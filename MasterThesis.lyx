#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass apa6
\begin_preamble
\usepackage[american]{babel}	
\usepackage{csquotes}
\usepackage[style=apa,natbib=true,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{/Users/jan/Studium/Masterarbeit/Thesis/MasterThesis.bib}
\AtBeginDocument{\renewcommand{\ref}[1]{\mbox{\autoref{#1}}}}
\end_preamble
\use_default_options true
\begin_modules
knitr
biblatex
logicalmkup
\end_modules
\maintain_unincluded_children false
\language american
\language_package none
\inputencoding utf8
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout ShortTitle
Recognition Memory
\end_layout

\begin_layout Title
Disentangling Cognitive Processes with Mathematical Modeling:
\emph on
 Evaluating Continuous and Discrete-State Models of Recognition Memory via
 Response-Scale and Encoding-Strength Manipulation
\end_layout

\begin_layout Author
Jan Vogt
\end_layout

\begin_layout Abstract
Abstract
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chunk

<<includes, echo=FALSE, eval=TRUE, chache=FALSE>>=
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/setup.R')
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/datahandling.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/fitting.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/modelspec.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/plotting.R')
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/chunks.R')
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<setup, echo=FALSE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<DataManipulation, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<ModelFitting, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<ModelSpecifications, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<Plotting, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Who has not experienced it: a perceived stranger approaches you with a warm
 smile and asks how are you as if you well known to each other.
 As awkward these situations are in real life as interesting is the research
 on the underlying process - recognition memory.
 Depending on your confidence in your recognition memory abilities, you
 might assume the foreigner to falsely recognize you or try to frantically
 remember where you had met this person and who it is.
 In most of these situations the truth will stay undetermined, but it is
 for sure, that for each role there are two possibilities.
 The person recognizing you can do that correctly, in recognition memory
 research this is called a 
\emph on
hit
\emph default
, or mistakenly, this is called a 
\emph on
false alert
\emph default
.
 You on the other hand not recognizing the person can be because it is a
 stranger, this is called 
\emph on
correct rejection
\emph default
, or you could be wrongly not recognize a known person, this is called a
 
\emph on
miss
\emph default
.
 But does the person recognizing you have a superior recognition memory
 ability or are you just more conservative in deciding wether you know one
 or not? Without any theory about the underlying processes this question
 can not be answered.
\end_layout

\begin_layout Standard
To generalize from this introductory example consider the other person from
 both peoples point of view as any item that could be known or not.
 Hit and miss translate identifying an known item as old or new, respectively
 and correct rejection and false alert translate to identifying an unknown
 item as new or old, respectively.
 The statistically experienced reader might easily see the fourfold pattern
 of two item types (known and unknown) and two response options (old and
 new) and conclude to answer the problem memory ability versus response
 bias via the overall probability to answer with old being the response
 bias and the odds ratio of answering old for known items over answering
 old for unknown ones as the memory ability.
 But although this would lead to an answer it is obvious that the parameter
 do not fulfill reasonable assumptions about the two processes.
 For example it it does not make sense that memory is dependent on response
 bias, but in this model it would be: Consider someone with perfect memory
 that is being able to remember everything.
 This person would identify every known item as known.
 But despite perfect memory the proposed odds ratio as memory indicator
 would vary based on that person's tendency to falsely identify unknown
 items as old, that is the memory indicator would increase as much as the
 person tends to identify items as unknown in general.
 A good memory indicator should identify the actual memory performance without
 being influenced by the individual preference for one or the other answer.
\end_layout

\begin_layout Standard
Mathematical modeling aims to disentangle these processes and by describing
 the underlying processes in a formal predictable fashion that allows to
 select competing models based on quantitative indicators, measure the influence
 of experimental treatments on specific subprocesses, and predict models
 idiosyncratic data patterns.
 This thesis uses all these abilities to compare two competing model classes
 for their ability to account for observed recognition memory data.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Standard
In the literature several models of recognition memory are discussed
\begin_inset CommandInset citation
LatexCommand citep
after "for a recent overview"
before "see"
key "snodgrass_pragmatics_1988,klauer_flexibility_2011"

\end_inset

.
 The most prominent ones are models based on the theory of signal detection
 
\begin_inset CommandInset citation
LatexCommand citep
key "green_signal_1966"

\end_inset

 and high threshold models 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g. "
key "blackwell_neural_1963,snodgrass_pragmatics_1988"

\end_inset

.
 The main difference of these models is the assumption underlying the nature
 of the memory process.
 However, both models inherently distinguish two types of processes, memory
 processes and response processes.
 That is, it is generally agreed upon that memory researchers need models
 to measure the quantities of interest.
 
\end_layout

\begin_layout Standard
Results of experiments involving a finite set of behavioral categories 
\begin_inset Formula $C_{1},C_{2},\ldots,C_{J}$
\end_inset

 can be described exhaustively as a set of each categories' observed frequency
 
\begin_inset Formula $D=\{N_{1},N_{2},\ldots,N_{j}\}$
\end_inset

.
 Under the assumption that these frequencies are mutually independent and
 identically distributed there is a probability 
\begin_inset Formula $p_{j}$
\end_inset

 for observing category 
\begin_inset Formula $C_{j}$
\end_inset

.
 The probability for finding a dataset under a given set of probabilities
 for each category is given as 
\begin_inset Formula $P(D\vert p_{1},p_{2},\ldots,p_{J})=\sum_{i=1}^{J}(N_{i})!\prod_{i=1}^{J}\frac{p_{i}^{N_{i}}}{N_{i}!}$
\end_inset

.
 Psychologists are usually interested in the generating cognitive processes
 of the observed data pattern 
\begin_inset Formula $D$
\end_inset

.
 Often the assumed processes and their relationships are described verbally,
 appropriate experimental manipulations are applied and corresponding changes
 in 
\begin_inset Formula $D$
\end_inset

 are hypothesized.
 If the null hypothesis is rejected, i.e.
 the probability for the found data in the experimental group under the
 assumption of equal probabilities for the categories to the control group
 is smaller than an acceptable error level 
\begin_inset Formula $P(D_{EG}\vert p_{1,}p_{2},\ldots,p_{J})<\alpha$
\end_inset

, the described process is seen as confirmed.
 Problems, which arises using this approach, are that as language is rather
 flexible it is often possible to explain unexpected results within the
 same processes and that interacting parts of the assumed processes can
 be difficult to link to the observable results.
 A possible solution to these problems is the use of a formal description
 of the assumed processes, i.e.
 a mathematical model linking the influence of an experimental manipulation
 to the observable data 
\begin_inset Formula $D$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Latent-Strength models
\begin_inset CommandInset label
LatexCommand label
name "sub:Latent-Strength-models"

\end_inset


\end_layout

\begin_layout Standard
Latent strength models assume that all stimuli vary along a familiarity
 scale.
 Its is usually assumed (although this is to some extent arbitrary) that
 the familiarity of items follows a normal distribution.
 When a set of items is presented to an individual during a study phase,
 it is assumed that this leads to an increase of familiarity.
 This familiarity increase for the studied items is represented by a distributio
n that assumes larger values on average (it is shifted to the right) than
 the distribution of non-studied items.
 Responses are made based on a cutoff criterion, that is items with familiarity
 above this criteria are classified as known and items below as unknown.
 Memory strength is captured as the amount the distribution of studied items
 is shifted.
 Response tendency is captured in the position of the cutoff criterion -
 the higher the cutoff criteria the lower the probability for classifying
 items as known (i.e.
 the more conservative the response bias).
 The most general form of this model has a parameter set 
\begin_inset Formula $\theta=\{\mu_{o},\mu_{n},\sigma_{o},\sigma_{n},c\}$
\end_inset

 with 
\begin_inset Formula $\{\mu_{o},\sigma_{o}\}$
\end_inset

 and 
\begin_inset Formula $\{\mu_{n},\sigma_{n}\}$
\end_inset

being the mean and standard deviation of old and new items, respectively,
 and 
\begin_inset Formula $c$
\end_inset

 being the response criterion.
 Without loss of generality, the mean and standard deviation of unknown
 items can be set to 0 and 1, respectively.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Phi(x)$
\end_inset

 denote the cumulative distribution function of the standard normal distribution
, the probability for a hit and a false alert are then given as
\begin_inset Formula 
\begin{align*}
P(\text{"old"}\vert\theta,\text{old item}) & =\Phi\left(\frac{c-\mu_{o}}{\sigma_{o}}\right)\\
P(\text{"old"}\vert\theta,\text{new item}) & =\Phi(-c)
\end{align*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
mu-c geändert in c-mu sonst dürfte es eigentlich nicht stimmen
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Responses to 
\begin_inset Formula $M$
\end_inset

-step bipolar confidence rating scales (
\begin_inset Formula $M$
\end_inset

 being an even integer), going from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure New
\emph default
) and 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Old
\emph default
), are assumed to be based on 
\begin_inset Formula $M+1$
\end_inset

 ordered cutoff criteria.
 The models parameter are then given as 
\begin_inset Formula $\theta=\{\mu,\sigma,c_{0},c_{1},\ldots,c_{M}\}$
\end_inset

, with 
\begin_inset Formula $c_{0}=-\infty<c_{1}<...<c_{M}=+\infty$
\end_inset

 .
 Let 
\begin_inset Formula $m$
\end_inset

 be the observed confidence rating (
\begin_inset Formula $1\leq m\leq M$
\end_inset

).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to an old and new item are respectively 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{old item}) & = & \Phi\left(\frac{c_{m}-\mu}{\sigma}\right)-\Phi\left(\frac{c_{m-1}-\mu}{\sigma}\right)\\
P(m\vert\theta,\text{new item}) & = & \Phi\left(c_{m}\right)-\Phi\left(c_{m-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In a two-alternative forced-choice (2AFC) task two items are presented on
 each trial (e.g., one item on the left and the other on the right side),
 and the individual's task is to indicate which of the two corresponds to
 the old item.
 It is assumed that individuals choose the item with the largest familiarity
 value.
 The model can conceptualized in terms of distributions representing the
 difference in familiarity between the two items, let us say the difference
 between the item on the left and the item on the right.
 When the old item is on the left, the familiarity difference follows a
 normal distribution with mean 
\begin_inset Formula $\mu$
\end_inset

 and standard deviation
\begin_inset Formula $\sqrt{1+\sigma^{2}}$
\end_inset

.
 When the old item is on the right, the familiarity difference follows a
 normal distribution with mean 
\begin_inset Formula $-\mu$
\end_inset

 and standard deviation 
\begin_inset Formula $\sqrt{1+\sigma^{2}}$
\end_inset

.
 We could also consider a case in which none of the items is old, a case
 in which the difference distribution has mean 0 and standard deviation
 of 
\begin_inset Formula $\sqrt{2}$
\end_inset

.
 Now, let us consider a 
\begin_inset Formula $M$
\end_inset

-step bipolar confidence rating scales going from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure Left
\emph default
) to 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Right
\emph default
).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to a left-old and right-old item are respectively 
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{left}) & = & \Phi\left(\frac{c_{m}+\mu}{\sqrt{1+\sigma^{2}}}\right)-\Phi\left(\frac{c_{m-1}+\mu}{\sqrt{1+\sigma^{2}}}\right)\\
P(m\vert\theta,\text{rigth}) & = & \Phi\left(\frac{c_{m}-\mu}{\sqrt{1+\sigma^{2}}}\right)-\Phi\left(\frac{c_{m-1}-\mu}{\sqrt{1+\sigma^{2}}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
c-mu für rechts alt und c+mu für links alt
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this thesis I will focus on the equal-variance SDT (
\begin_inset Formula $\text{EVSDT}$
\end_inset

) model, in which
\begin_inset Formula $\sigma_{o}$
\end_inset

 is also fixed to 1.
\end_layout

\begin_layout Subsection
Discrete-State models
\begin_inset CommandInset label
LatexCommand label
name "sub:Discrete-State-models"

\end_inset


\end_layout

\begin_layout Standard
Discrete-state models assume that observed responses result from a mixture
 of discrete mental states which are entered or not depending on the probabilist
ic occurrence of specific cognitive processes.
 In a old-new recognition task, it is assumed that old items either enter
 a certainty state (
\begin_inset Formula $S_{1}$
\end_inset

) with probability 
\begin_inset Formula $d_{o}$
\end_inset

 or an uncertainty (
\begin_inset Formula $S_{2}$
\end_inset

) state with probability 
\begin_inset Formula $1-d_{o}$
\end_inset

.
 When in state 
\begin_inset Formula $S_{1}$
\end_inset

, the old items are invariably recognized (response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

).
 New items enter certainty state 
\begin_inset Formula $S_{3}$
\end_inset

 with probability 
\begin_inset Formula $d_{n}$
\end_inset

 or the uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

 with probability 
\begin_inset Formula $1-d_{n}$
\end_inset

.
 New items in state 
\begin_inset Formula $S_{3}$
\end_inset

 are invariably rejected (response 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

).
 When items enter the uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

, their status cannot be ascertained and hence a guessing process ensues.
 Response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 is given with probability 
\begin_inset Formula $g$
\end_inset

 and response 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

 is given with probability 
\begin_inset Formula $1-g$
\end_inset

.
 The probability of response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 for old and new items is given respectively by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(\text{"old"}\vert\theta,\text{old item}) & = & d_{o}+(1-d_{o})g\\
P(\text{"old"}\vert\theta,\text{new item}) & = & (1-d_{n})g
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The extension of this so called 2 high-threshold model (2HTM) to confidence-rati
ng scales requires the specification of state-response mapping functions
 that assign the probability of each rating response, conditional on the
 discrete state.
 Let 
\begin_inset Formula $\delta_{o}$
\end_inset

, 
\begin_inset Formula $\delta_{n}$
\end_inset

,
\begin_inset Formula $\gamma_{o}$
\end_inset

, and 
\begin_inset Formula $\gamma_{n}$
\end_inset

 be probability vectors with length 
\begin_inset Formula $M$
\end_inset

, each vector summing to 1.
 Vectors 
\begin_inset Formula $\delta_{o}$
\end_inset

 and 
\begin_inset Formula $\delta_{n}$
\end_inset

 are assigned to states 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{3}$
\end_inset

 respectively, and attribute probability 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary response.
 Similarly, vectors 
\begin_inset Formula $\gamma_{o}$
\end_inset

 and 
\begin_inset Formula $\gamma_{n}$
\end_inset

 attribute probability 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary response (
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 with probability 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

 with probability 
\begin_inset Formula $1-g$
\end_inset

).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to an old and new item are respectively
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{old item}) & = & d_{o}\delta_{o.m}+(1-d_{o})\bigl(g\gamma{}_{o,m}+(1-g)\gamma{}_{n,m}\bigr)\\
P(m\vert\theta,\text{new item}) & = & d_{n}\delta_{n,m}+(1-d_{n})\bigl(g\gamma{}_{o,m}+(1-g)\gamma{}_{n,m}\bigr)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In 2AFC tasks both detection states 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{3}$
\end_inset

 lead invariable to the correct answer (e.g.
 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 independently if the right item is being recognized as the old one or the
 left one as the new).
 Thus 
\begin_inset Formula $d_{o}$
\end_inset

 and 
\begin_inset Formula $d_{n}$
\end_inset

 are not separable and thus replaced by 
\begin_inset Formula $d_{l}$
\end_inset

, and 
\begin_inset Formula $d_{r}$
\end_inset

, representing the probabilities of entering any detect state if the old
 item is presented on the left or right side, respectively.
 Formally they can be understood as 
\begin_inset Formula $d_{l}=1-(1-d_{o})(1-d_{n})$
\end_inset

 if the old item is presented on the left side and 
\begin_inset Formula $d_{r}$
\end_inset

 being the equivalent for the right side.
 The uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

 leads to guessing wether the old item is left or right with 
\begin_inset Formula $g$
\end_inset

 being the probability for a response 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Formula $1-g$
\end_inset

 for 
\begin_inset Quotes eld
\end_inset

left
\begin_inset Quotes erd
\end_inset

.
 Similar to the yes-no confidence-rating task we define 
\begin_inset Formula $\delta_{l}$
\end_inset

, 
\begin_inset Formula $\delta_{r}$
\end_inset

, 
\begin_inset Formula $\gamma_{l}$
\end_inset

, and 
\begin_inset Formula $\gamma_{r}$
\end_inset

 as the state-response mapping.
 
\begin_inset Formula $\delta_{l}$
\end_inset

 and 
\begin_inset Formula $\delta_{r}$
\end_inset

 are mapping detect states on the left and right side, respectively, to
 rating 
\begin_inset Formula $m$
\end_inset

 with probability 0 for the other side.
 And 
\begin_inset Formula $\gamma_{l}$
\end_inset

, and 
\begin_inset Formula $\gamma_{r}$
\end_inset

 attribute probabilities 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary 
\begin_inset Quotes eld
\end_inset

left
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answer, respectively.
 In cases where both items are new a certainty state cannot be reached and
 therefore 
\begin_inset Formula $d_{l}=d_{r}=0$
\end_inset

.
 Considering the 
\begin_inset Formula $M$
\end_inset

-step scale from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure left
\emph default
) to 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Left
\emph default
),
\emph on
 
\emph default
the probability of response 
\begin_inset Formula $m$
\end_inset

 being given an left-old and right-old item are respectively:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{left}) & = & d_{l}\delta_{l.m}+(1-d_{l})\bigl(g\gamma{}_{r,m}+(1-g)\gamma{}_{l,m}\bigr)\\
P(m\vert\theta,\text{right}) & = & d_{r}\delta_{r,m}+(1-d_{r})\bigl(g\gamma{}_{r,m}+(1-g)\gamma{}_{l,m}\bigr)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In this thesis I will focus on the 1 high-threshold model (
\begin_inset Formula $\text{1HTM}$
\end_inset

), in which detection states are set to be equal for both sides 
\begin_inset Formula $d_{l}=d_{r}$
\end_inset

 and state-response mappings are mirrored on the response-scale's center
 
\begin_inset Formula $\delta_{l,m}=\delta_{r,M-m+1}$
\end_inset

 and 
\begin_inset Formula $\gamma_{l,m}=\gamma_{l,M-m+1}$
\end_inset

.
 For simplicity we speak only parameter 
\begin_inset Formula $d=d_{l}=d_{r}$
\end_inset

, 
\begin_inset Formula $\delta_{l}$
\end_inset

, and 
\begin_inset Formula $\gamma_{m}=\bigl(g\gamma{}_{r,m}+(1-g)\gamma{}_{l,m}\bigr)$
\end_inset

 as the model is thereby fully specified.
\end_layout

\begin_layout Subsection
Model selection
\end_layout

\begin_layout Standard
Consider we have 
\begin_inset Formula $n$
\end_inset

 competing models 
\begin_inset Formula $M=\{m_{1},m_{2},\ldots,m_{n}\}$
\end_inset

, which is the best? To answer this question we need to define what 
\emph on
best
\emph default
 means.
 Whereas it is not trivial to state what is best it is easy to state what
 not.
 If we define the best model as the one that describes the data the closest,
 that is the found data is most likely if this model is true: 
\begin_inset Formula $P(D\vert m_{best})=\max\left(\bigcup_{m\in M}P(D|m)\right)$
\end_inset

 then the best model we could define is the data itself.
 But without any information reduction we would not need any model after
 all.
 Defining the best model as the one which reduces the information the most
 is not reasonable because the perfect case would be to say nothing at all
 about the data, that is we would not have needed even to conducted the
 research if that would be it's goal.
 It becomes clear that 
\emph on
best
\series bold
 
\series default
\emph default
has to be a tradeoff between a models ability to fit data and it's flexibility.
 The much fit as possible with as much flexibility as necessary.
\end_layout

\begin_layout Standard
It follows immediately that if model 
\begin_inset Formula $m_{a}$
\end_inset

 has less flexibility and greater fit than 
\begin_inset Formula $m_{b}$
\end_inset

, the latter can not be the best model.
 But this leaves to open problems: (a) how is flexibility measured in general
 and (b) how is this measure related to fit, which is important to select
 models in cases with trade-offs, for example model 
\begin_inset Formula $m_{a}$
\end_inset

 has better fit but also greater flexibility than 
\begin_inset Formula $m_{b}$
\end_inset

.
 To measure flexibility various methods have been proposed.
\end_layout

\begin_layout Quotation
Some of the principled techniques in use are cross-validation methods, bootstrap
 simulations to assess model mimicry, and the use of model-selection indices
 such as AIC and BIC.
 Model-selection indices are used most frequently in applications of the
 present [recognition memory] models.
 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 431"
key "klauer_flexibility_2011"

\end_inset


\end_layout

\begin_layout Standard
Mathematically it is most reasonable to measure flexibility as the ability
 of a model to account for every possible data pattern.
 This understanding is reflected in indices based on the principle of minimum
 description length (MDL such as the normalized maximum likelihood index
 
\begin_inset CommandInset citation
LatexCommand citep
before "NML;"
key "myung_model_2006"

\end_inset

.
 These indices are used only rarely because they are difficult to compute
 even for modern computers.
 For recognition memory they are just recently applied to binary response
 data
\begin_inset CommandInset citation
LatexCommand citep
key "klauer_flexibility_2011"

\end_inset

but have yet to be extended for confidence ratings.
 It can be said that current state-of-art techniques are the AIC and BIC
 indices, which evaluate flexibility primarily on the number of a models
 parameter.
 This does not take into account that parameter can vary in their influence
 to the predicted data, such as cognitive processes can vary in their influence
 to observed data.
 AIC and BIC overly punish models which have parameters that yield only
 small influences to the predicted data and thus selections made on the
 basis of AIC and BIC tend to choose models which have parameters that have
 great influence on predicted data.
\end_layout

\begin_layout Standard
Regarding the two models under consideration in this work this seems to
 be the case.
 The discrete-state models, such as the 1HTM, constantly yield good but
 worse fit than the continuous models, such as EVSDT.
 Having the same number of parameters AIC and BIC turn out in favor of of
 the continuous models.
 Summarizing these results many concluded that discrete-state models do
 not account adequately for recognition memory data 
\begin_inset CommandInset citation
LatexCommand citep
key "yonelinas_receiver_2007,wixted_dual-process_2007"

\end_inset

.
 But as we know from more sophisticated flexibility measures 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g. "
key "klauer_flexibility_2011"

\end_inset

 discrete state models are less flexible in their functional form - so it
 might be that the lack of fit is due to the smaller flexibility.
 So it could be that the actual underlying processes are superiorly captured
 by discrete-state models and the predominance of continuous models is due
 to their ability to fit random data.
 Recent findings underpin these considerations: 
\begin_inset CommandInset citation
LatexCommand citet
key "broder_recognition_2009"

\end_inset

 showed that for payoff or base-rate manipulated response biases discrete-state
 models fare better then continuous models.
 For confidence ratings 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 showed that the tenuous assumption that reaching a detect state leads to
 highest confidence responses (certainty assumption) could be the reason
 for discrete-state models' inferiority.
 They suggest 
\emph on
conditional independence
\emph default
, that is once a certain state is entered the distribution for responses
 is determined.
 Using this principle they showed strong evidence in favor of discrete-state
 models.
\end_layout

\begin_layout Subsection
Interpreting recent findings
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

's
\begin_inset CommandInset citation
LatexCommand citeyearpar
key "province_evidence_2012"

\end_inset

 paper was groundbreaking for the new consideration of discrete-state models
 for recognition memory.
 But the used methodology has some peculiarities which complicate comparing
 their results with earlier findings.
 Namely their use of continuous rating scales with post-hoc binning in experimen
t one and two.
 The asymmetric payoff scheme, which aimed to let participants scale their
 responses more towards the center, with immediate feedback in experiment
 two.
 Usage of highly associated words as distractors in the 2AFC task in experiment
 three.
 Although it is unclear in which way these features contributed to their
 results, theoretically the conditional independence should be sufficient.
 The main goal of this thesis is thus to replicate 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

's findings with a more comparable paradigm.
\end_layout

\begin_layout Standard
Additionally experimental manipulation of the assumed processes should on
 one hand be reflected in the corresponding parameters and on the other
 hand can decrease models with wrong assumptions fit.
 Assuming that the increased fit should also be attributed to the correctness
 of the model, encoding strength as well as mapping is manipulated.
\end_layout

\begin_layout Standard
The best model is not necessarily the one that better s the data, as it
 is also important that a model's range of predictions closely follows the
 observations made and that it can produce accurate predictions regarding
 future observations (Roberts & Pashler, 2000) (for discussions on derent
 model selection approaches, see Myung, Forster, & Browne, 2000;Wagenmakers
 & Waldorp, 2006),
\end_layout

\begin_layout Subsection
Hypothesis
\end_layout

\begin_layout Enumerate
Manipulation of encoding strength shows increase in correct responses.
 These changes should be captured by memory performance parameters (congruent
 validity) of the models under consideration and should not influence any
 other parameters divergent validity.
 Thus, for the two different encoding strengths in all experiments it is
 hypothesized to change the memory parameter (i.e.
 
\begin_inset Formula $\mu$
\end_inset

, 
\begin_inset Formula $d$
\end_inset

 in EVSTD and 1HTM, respectively).
 (No influence on others? Test?)
\end_layout

\begin_layout Enumerate
Manipulation of the response scale should affect the mapping of responses.
 These changes should be captured by response mapping parameters (congruent
 validity) of the models under consideration and not influence any other
 parameter (divergent validity).
 Thus, for the two dofferent response scales in the first sessions it is
 hypothesized to change the response mapping parameter (i.e.
 
\begin_inset Formula $c_{1},\ldots,c_{\frac{n}{2}-1},c_{\frac{n}{2}+1},\ldots,c_{n-1}$
\end_inset

; 
\begin_inset Formula $p_{d,1},\ldots,p_{d,\frac{n}{2}-1},\ldots,p_{g_{r},\frac{n}{2}-1}$
\end_inset

 in EVSDT and 1HTM, respectively).
 Guessing (i.e.
 
\begin_inset Formula $c_{\frac{n}{2}}$
\end_inset

, 
\begin_inset Formula $g_{r}$
\end_inset

 in EVSDT and 1HTM, respectively) bias should be unaffected.
\end_layout

\begin_layout Enumerate
Better fit for 1HTM found by
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

should be replicated.
 1HTM model should yield better fit than EVSDT
\end_layout

\begin_layout Standard
Explorative:
\end_layout

\begin_layout Enumerate
Differences in memory parameters within subject between words and line drawings
 as memorized items.
 Are there differences in other parameters as well?
\end_layout

\begin_layout Enumerate
Role of the zero repetition trials, that are responses to trials that had
 two new words in an 2AFC trial.
\end_layout

\begin_layout Enumerate
Model mimicry of EVSDT and 1HTM
\end_layout

\begin_layout Enumerate
Model landscaping of EVSDT and 1HTM
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
Is it the maximal probability for the observed data? If so, then the probability
 estimates from observed data is the best.
 But they are of little interest for other data then the particular dataset
 they were obtained from.
 
\end_layout

\begin_layout Plain Layout
Disentangeling relevant processes is key.
 -> generalizable, paremter of relevance, 
\end_layout

\begin_layout Plain Layout
So being a good model has something to do with generalizability.
 But what? Or what is generalizability? Is it the predictions for other
 data sets? In general yes, but as parameter estimates are allowed to vary
 these predictions are very broad.
 So we need smaller confidence intervals of the parameter? that certainly
 makes sense.
 
\end_layout

\begin_layout Plain Layout
Is it how the models extract the underlying processes from the data? How
 can we messure how good that is -> parameter validation.
 
\end_layout

\begin_layout Plain Layout
Models should also condense data.
 Why? Because the more flexible a model is the greater the probability of
 having irrelevant parameter.All parameters should be properly validated
 -> scale manipulation for response mapping.
\end_layout

\begin_layout Plain Layout
The risk of having irrelevant parameters could be extended to functional
 form of the model.
 the number of parameters is not alone crucial to the model flexibility.
 
\end_layout

\begin_layout Plain Layout
Mode flexibility is difficult to measure.
 MDL being mathematically the best method.
 model landscaping, model mimicry are indicators for flexibility in comparison
 to other models.
 
\end_layout

\begin_layout Plain Layout
model flexibility is especially important when models with good fit are
 also assumed to be more flexible.
\end_layout

\begin_layout Plain Layout
Very convincing arguments are when the less flexible model obtains better
 fits.
 Especially when the parameters are valid - but because parameter assumptions
 can be wrong that is less important.
\end_layout

\begin_layout Plain Layout
this is what province rouder showed.
\end_layout

\begin_layout Plain Layout
Bot presented models have, so far, proofed to be good account for recognition
 memory data.
 Models are designed to disentangle cognitive processes' underlying observed
 data.
\end_layout

\begin_layout Plain Layout
Model appropriateness is often based on model fit and parsimony.
\end_layout

\begin_layout Plain Layout
SDT models are more flexible but also fit the data better.
\end_layout

\begin_layout Plain Layout
but newer arguments challenge these findings 
\begin_inset CommandInset citation
LatexCommand cite
key "broder_recognition_2009"

\end_inset

 and these arguments are supported by emprical data from 
\begin_inset CommandInset citation
LatexCommand cite
key "province_evidence_2012"

\end_inset

.
\end_layout

\begin_layout Plain Layout
...
\end_layout

\begin_layout Plain Layout
Experimental manipulations of the assumed processes should map to the to
 the respective parameters of the model.
\end_layout

\begin_layout Plain Layout
In recognition memory models response processes are manipulated (Citations)
\end_layout

\begin_layout Plain Layout
This can be done by obtaining confidence ratings for the classifications:
 the proportion of hits per false alerts increases with higher confidence.
 That is participants are more conservative with their answers with increasing
 confidence.
\end_layout

\begin_layout Plain Layout
two alternative forced choice (2AFC) rating scales are a straightforward
 extension to this principle.
 
\end_layout

\begin_layout Plain Layout
Response mapping manipulation
\end_layout

\begin_layout Plain Layout
Encoding strength manipulation via time or repeated presentation+
\end_layout

\begin_layout Plain Layout
Description of models
\end_layout

\begin_layout Plain Layout
All models are making strong assumptions about underlying and unobservable
 processes generating the observable data.
 Additionally to the likelihood for the found data given a specific model
 it is necessary to evaluate the models ability to appropriately reacting
 to treatment manipulation aiming at specific parameters of the models.
\end_layout

\begin_layout Plain Layout
Models differ in their ability to account for data
\end_layout

\begin_layout Plain Layout
Necessity for more data point (ROC plots)
\end_layout

\begin_layout Plain Layout
Options: besrate payoff responce scale
\end_layout

\begin_layout Plain Layout
Responce scale options yes/no vs.
 2AFC
\end_layout

\begin_layout Plain Layout
models
\end_layout

\begin_layout Plain Layout
.
 
\end_layout

\begin_layout Plain Layout
In recognition memory research it is possible to categorize the models in
 three different classes: 
\emph on
discrete-state models
\emph default
, 
\emph on
continuous models,
\emph default
 and 
\emph on
combined models
\emph default
.
 These classes of models are not specific to recognition memory, but are
 applied to a wide field of cognitive research.
 Namely their use of continuous and post-hoc binned rating scales in experiment
 one and two, the asymmetric response scale manipulation in experiment two
 and the use of highly associated words as distractors in the 2AFC task
 in experiment three complicate the interpretation of their results.
 The central research goal in this work is thus to replicate their findings
 with a more comparable design.
\end_layout

\begin_layout Subsection
(Fitting process)
\end_layout

\begin_layout Plain Layout
Model fitting means finding the parameters for the models that maximise
 the likelyhood of the found data.
\end_layout

\begin_layout Plain Layout
The models parameter can be estimated basted on the observed data.
\end_layout

\begin_layout Plain Layout
Estimation is based on maximizing the likelihood for the the given data
 under the fitted model.
\end_layout

\begin_layout Plain Layout
need for a likelihood function for data and model parameter 
\begin_inset Formula $L(D;\theta)$
\end_inset


\end_layout

\begin_layout Plain Layout
maximizing the likelihood function leads to maximum likelihood parameter
 estimates 
\begin_inset Formula $\hat{\theta}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $L(D;\hat{\theta})$
\end_inset

 is the goodness of fit indicator for that model.
\end_layout

\begin_layout Plain Layout
Good fit can be achieved by the appropriateness of the model or by flexible
 models.
\end_layout

\begin_layout Subsection
Model Fit
\end_layout

\begin_layout Plain Layout
Model fit is the likelihood for the data under the best parameter estimates
 of the model.
 
\end_layout

\begin_layout Plain Layout
A better model fit means that the model is the better account for the observed
 data pattern.
\end_layout

\begin_layout Plain Layout
But models 
\end_layout

\begin_layout Plain Layout
fitting process
\end_layout

\begin_layout Plain Layout
model comparison
\end_layout

\begin_layout Plain Layout
latent strength vs.
 discrete state
\end_layout

\begin_layout Plain Layout
genereal
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 are great! 
\end_layout

\begin_layout Plain Layout
weaknesses of 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset


\end_layout

\begin_layout Plain Layout
hypothesis for this thesis
\end_layout

\begin_layout Plain Layout
Province and Rouder's
\begin_inset CommandInset citation
LatexCommand citeyearpar
key "province_evidence_2012"

\end_inset

 results show evidence against the widely accepted superiority of continuous
 models.
 But on the other hand they used some specialities in their studies which
 decrease comparability of their results considerably.
 
\end_layout

\begin_layout Plain Layout
Role of the forced guessing condition
\end_layout

\begin_layout Plain Layout
scale manipulation
\end_layout

\begin_layout Plain Layout
Recognition memory is the ability to recognize previously seen items as
 such.
 The central principle is that there are four response options: hit, false
 alert, correct rejection and miss.
 It is not trivial to separate actual memory performance from the tendency
 to answer in a specific way.
 It is necessary to determine the way these processes are interlinked to
 produce the observable result.
 This is done using mathematical models linking different treatments to
 the respective outcomes.
 
\end_layout

\begin_layout Plain Layout
One class of models are the discrete state models
\end_layout

\begin_layout Plain Layout
2AFC if both are detected as new? What to answer?
\end_layout

\begin_layout Plain Layout
Pictures are easier to learn and recognize than words (Paivio, 1971, 1986).
\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Chunk

<<Demographics, echo=FALSE>>= 
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
Two experiments are conducted, the first being a more classical version
 without the use of a zero repetition condition.
 The second one is a close replication of the first one with key change
 being the addition of the zero repetition condition.
 In both Experiments two sessions are conducted: The first with words and
 the second one with line drawings as the stimuli to be remembered, respectively.
 All experiments being conducted in the Department for Social Psychology
 and Methodology in the University of Freiburg.
 All participants have written consent to take part and their data being
 published.
 To anonymize the date but combine the two sessions every participant is
 asked to generate an individual non traceable code.
 They also could opt-in to receive their individual data by giving their
 e-mail addresses, which where separated from the data set prior to analysis.
\end_layout

\begin_layout Subsection
Experiment 1
\end_layout

\begin_layout Standard
This experiment is designed to be a close replication of 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

, with the addition of scale manipulation and a second session with pictures
 as memorized stimuli.
 Also all unusual characteristics of 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

's experiments were omitted, namely the use of continuous and post-hoc binned
 rating scales, the use of associated words as targets and lures, the scale
 manipulation with stronger penalization for wrong responses and immediate
 feedback, and the zero repetition condition.
\end_layout

\begin_layout Subsubsection
Subjects
\end_layout

\begin_layout Standard
Twenty-three participants (female: 17, male: 6, 
\emph on
M
\emph default
=22.8, range: 19-31), mostly psychology students (
\emph on
n
\emph default
=21), were recruited in university courses, via e-mail list, and an notice
 on a bulletin board.
 They were offered the chance to get 50 €, 30 €, or 20 € for achieving the
 first, second, or third best result, respectively.
 The challenge (i.e.
 background story) was to evaluate certainty in recognition.
 It was stressed that this is a different concept to their mere ability
 to for recognition.
 Additionally traditional VP-Stunden
\begin_inset Foot
status open

\begin_layout Plain Layout
VersuchsPersonen Stunden: Psychology students have to take part in research
 and get in exchange a recipe for their expenditure of time.
\end_layout

\end_inset

, their individual results and a copy of the final thesis was offered.
 Eighteen participated in both sessions the remaining 5 only in Session
 1.
\end_layout

\begin_layout Subsubsection
Materials
\end_layout

\begin_layout Standard
All tests were run on a Windows computer using a custom build Python program.
 Word stimuli for Session 1 were neutral german nouns taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "lahl_using_2009"

\end_inset

 ranging from 4 to 8 letters in length.
 According to the ratings obtained by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "lahl_using_2009"

\end_inset

, the words were all of medium valence (range: 3.5-6.5 on an 11-point scale)
 and low in arousal (range: 0.5-4.5 on an 11-point scale).
 Out of this collection 644 nouns that were of approximately equal word
 frequency in common German, as indicated by log frequency ratings obtained
 for each word via WordGen 
\begin_inset CommandInset citation
LatexCommand citep
before "ranging from 0.3 to 2.9;"
key "duyck_wordgen:_2004"

\end_inset

, were selected.
 Pictures stimuli for Session 2 were 525 black and white line drawings that
 were obtained from 
\begin_inset CommandInset citation
LatexCommand cite
key "szekely_new_2004"

\end_inset

.
\end_layout

\begin_layout Standard
Confidence ratings for the 2AFC trials were collected using an 8-step scale.
 Each half of the scale was labeled 
\emph on
Left
\emph default
 or 
\emph on
Right
\emph default
, respectively.
 In two scale types, differing in the amount of points, were used: (a) a
 
\begin_inset Quotes eld
\end_inset

low-risk
\begin_inset Quotes erd
\end_inset

 scale ranging from 
\emph on
1
\emph default
 to 
\emph on
4
\emph default
 points in steps of 1 and (b) a 
\begin_inset Quotes eld
\end_inset

high-risk
\begin_inset Quotes erd
\end_inset

 scale ranging from 
\emph on
1
\emph default
 to 
\emph on
13 
\emph default
points in steps of 4, for the most meidal to the most lateral buttons on
 each side, respectively.
 Each scale was presented below the two stimuli of the 2AFC task with each
 stimuli being centered above the it's respective half of the scale (e.g.
 the 
\emph on
left
\emph default
 stimuli above the left half, see
\emph on
 
\emph default

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rating-scales"

\end_inset

).
 One of the two stimuli was previously memorized (target) and the other
 was new (lure).
 Depending on the position of the previously memorized stimuli the trial
 is called left-old or right-old if the target is on the left or right position,
 respectively.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Rating scales
\begin_inset CommandInset label
LatexCommand label
name "fig:Rating-scales"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Procedures
\end_layout

\begin_layout Standard
Participants were asked to take part in both sessions with at least one
 week interval between.
 The first session consisted of four, the second session of two blocks.
 Each block consisted of three phases: (1) The learning phase in which participa
nts were presented the stimuli to be memorized for this block, (2) a retention
 phase with easy mental arithmetics for 5 min., and (3) the recall phase
 in which the items memorized for this block were retrieved.
 Encoding strengths were manipulated in both sessions and response scales
 were manipulated only in the first session.
 Participants were informed about their tasks in these three phases.
 Also they were informed that they will be able to collect points in the
 retention phase and recall phase.
\end_layout

\begin_layout Paragraph
Session 1
\end_layout

\begin_layout Standard
For each participant 2 x 288 words were randomly chosen to act as targets
 and lures, respectively.
 The remaining words acted as filler for primacy and recency stimuli.
 In each of the four learning phases 72 of the target stimuli were shown.
 Half of these items were randomly chosen to be 
\begin_inset Quotes eld
\end_inset

strong
\begin_inset Quotes erd
\end_inset

 encoded the other half to be 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 encoded.
 These items were then shown 4 or 1 time, respectively.
 Additionally ten filler items to account for primacy and recency effects
 were shown as the first five and last five stimuli.
 This sums to 190 words being presented in each learning phase.
 Each word was shown for 400 msec.
 with an inter stimulus interval (ISI) of 200 msec.
\begin_inset Foot
status open

\begin_layout Plain Layout
Timings were adjusted to minimize empty cells (i.e.
 medium memory performance)
\end_layout

\end_inset

.
 The words were presented in a random fashion that ensures that at least
 five different words have been shown between two repetitions of the same
 word.
 Participants were instructed to memorize these words in order to recognize
 them later on and were told that word could be presented multiple times.
\end_layout

\begin_layout Standard
The retention phase consisted of randomly generated easy mental arithmetics.
 For each correct answer participants gained one point and wrong answers
 had to be corrected.
 After five minutes the phase was finished with the next answer irregardless
 wether it was correct or not.
\end_layout

\begin_layout Standard
In the recall phase all 72 words presented in the learning phase were presented
 as 2AFC trials.
 For each block either the high-risk or low-risk scale was selected randomly
 with the restriction that the first two blocks had different scales and
 each scale type was used twice.
 Participants were instructed that they had to identify in which side the
 previously learned item was presented and that they will gain or loose
 the selected amount of points depending on wether they choose the side
 were the target is shown or not.
 As a reference for the points' value each participant gained 100 point
 at the beginning of each recall phase.
 To prevent motivational issues no feedback was given during both sessions.
 But it repeatedly stressed that the three participant with the highest
 overall points get the monetary reward.
\end_layout

\begin_layout Paragraph
Session 2
\end_layout

\begin_layout Standard
For each participant 2 x 248 line-drawings were randomly chosen as targets
 and lures.
 Similar to Session 1 encoding strength was manipulated with 
\begin_inset Quotes eld
\end_inset

strong
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 encoded pictures being shown for 3 and 1 time respectively.
 Together with the ten primacy and recency filler pictures this evaluates
 to 258 pictures presented in each of the two learning phases.
 Presentation times were 250 msec.
 with the same ISI of 200 msec.
 as in Session 1.
 Retention and recall phase were conceptually identical with 124 pictures
 being rated in each block.
 The only difference was that in the recall phase the scales were not manipulate
d and only the low-risk scale was used for both blocks.
\end_layout

\begin_layout Paragraph
Socio-demographics
\end_layout

\begin_layout Standard
At the end of each session - to minimize social priming (source) - socio-demogra
phic data was collected or reconfirmed in Session 1 and 2, respectively.
 Age, sex, occupation, sight problems, color blindness, and optionally comments
 and e-mail were collected.
\end_layout

\begin_layout Subsubsection
Design
\end_layout

\begin_layout Standard
The key treatment factors were Encoding, Scale, and Position the respective
 levels are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:experimental-factors"

\end_inset

.
 Encoding strength was manipulated item-wise in both Sessions via different
 repetition conditions for memorized stimuli.
 Scale was manipulated block-wise and only in Session 1 via different points
 on the rating scale.
 Position was manipulated item-wise via the position of the old word in
 the 2AFC rating task.
 This evaluated to a 2 x 2 x 2 and 2 x 1 x 2 within participants factorial
 design for Session 1 and 2, respectively.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Experimental design for both experiments.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:experimental-factors"

\end_inset


\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="13">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Exp.
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Session
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Encoding
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Position
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Scale
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
factor comb.
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
steps
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cells
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
df
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
trials
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Verbal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 4x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low, high)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
36
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
56
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Visual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 3x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Verbal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(0x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(no-old)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low, high)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
36
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 4x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Visual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(0x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(no-old)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 3x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiment 2
\end_layout

\begin_layout Standard
This Experiment is designed to be a replication of Experiment one with the
 addition of the zero repetition condition used by 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 which turned out to be crucial to differentiate between the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 and the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

 model.
 The sole additional change was the use of a 6-step rating scale since the
 medium two steps for each half of the 8-step scale in Experiment 1 sometimes
 led to empty cells making it difficult to properly estimate their empirical
 probabilities.
\end_layout

\begin_layout Subsubsection
Subjects
\end_layout

\begin_layout Standard
Thirty-three participants were recruited by same means as used for Experiment
 1.
 This led to comparable characteristics (female: 28, male: 5, 
\begin_inset Formula $ $
\end_inset


\emph on
M
\emph default
=21.4, range: 18-34) and 
\emph on
n
\emph default
=31 being psychology students.
 The incentives and background story were identical to Experiment one.
 It was not possible for participants to know in which experiment they took
 part.
 Thirty participants participated in both sessions and the the remaining
 three only in Session 1.
\end_layout

\begin_layout Subsubsection
Materials
\end_layout

\begin_layout Standard
The same Python program running on the same computers was used.
 Word stimuli were selected form the same pool obtained from 
\begin_inset CommandInset citation
LatexCommand citet
key "lahl_using_2009"

\end_inset

 but in order to keep memory constant, additional words for the zero repetition
 condition were necessary.
 Thus 857 nouns with also approximately equal word frequency in common German,
 as indicated by frequency classes obtained for each word from DeReWo
\begin_inset CommandInset citation
LatexCommand citep
before "ranging from 10 to 16; "
key "institut_fur_deutsche_sprache_programmbereich_korpuslinguistik_korpusbasierte_2013"

\end_inset

.
 Pictures were the same525 taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "szekely_new_2004"

\end_inset

 as in Experiment 1.
 The 2AFC trials were almost identical to Session 1 only being reduced to
 6-steps.
 Thus the low-risk scale ranged from 
\emph on
1
\emph default
 to 
\emph on
3
\emph default
 points and the high-risk scale ranged from 
\emph on
1
\emph default
 to 
\emph on
9 
\emph default
points, from the medial to the lateral buttons (see
\emph on
 
\emph default

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rating-scales"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Procedures
\end_layout

\begin_layout Standard
The instructions, the overall experimental procedures (e.g.
 three phases per block; 4, 2 block in Session 1 and 2, respectively), and
 the timing was identical to Experiment 1.
 The sole exception was the inclusion of a zero repetition condition in
 both Sessions.
\end_layout

\begin_layout Paragraph
Session 1
\end_layout

\begin_layout Standard
For each participant 2 x 360 words were randomly selected to act as old
 and new words, respectively.
 In each of the four blocks 90 of the old words were considered as memorized,
 but only 72 were actually presented 4 or 1 times as strong and weak encoded
 words, respectively (i.e.
 the learning phase was identical to the corresponding one in Experiment
 1).
 The remaining 18 words in each block were not shown in the memory phase,
 thus they are labeled them as 0 times shown.
 In the 2AFC task all 90 items per block were rated, but the position of
 the 0 times shown words was unimportant, as both items were new to the
 participants.
 Thus the 90 2AFC trials in each block were equally divided in five types:
 two left-old and two right-old for each 1 x and 4 x repetition condition,
 respectively, and one no-old for the 0 x repetition condition.
\end_layout

\begin_layout Paragraph
Session 2
\end_layout

\begin_layout Standard
For each participant 2 x 250 pictures were randomly selected to act as old
 and new images, respectively.
 In each of the two blocks 125 of the old pictures were considered as memorizes,
 but only 100 were actually presented 3 or 1 times as strong and weak encoded
 pictures, respectively (i.e.
 due to the restricting size of the used picture pool, participants had
 to memorize 24 pictures less in each block compared to Experiment 1, which,
 in turn, led to a slight but unproblematic increase in correct answers).
 The remaining 25 pictures in each block used as 0 times shown items.
 Thus, the 125 items per block were equally across the five types of 2AFC
 trials in the recall phase.
\end_layout

\begin_layout Subsubsection
Design
\end_layout

\begin_layout Standard
The design had the same factors as Experiment 1 although with additional
 levels to account for the addition of the zero repetition condition (see
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:experimental-factors"

\end_inset

).
 The Scale manipulation was applied block-wise and exclusively in Session
 1.
 Encoding strengths and Positions were varied item-wise with the no-old
 position being the only position for the 0 x encoding strength level and
 left-old and right-old position being nested under the weak and strong
 levels of the Encoding factor.
 This evaluates to (2 x 2 + 1 x 1) x 2 and (2 x 2 + 1 x 1) x 1 factor combinatio
ns for Encoding, Position, and Scale factors in Session 1 and 2, respectively.
\end_layout

\begin_layout Subsection
Model Specification
\end_layout

\begin_layout Standard
The most general versions and three restricted versions, respectively, of
 the discrete-state model and the continuous model are used to analyze the
 data.
 In the following they will be defined one by one.
\end_layout

\begin_layout Subsubsection
Discrete-state models
\end_layout

\begin_layout Standard
In this thesis I focus on the 1HTM as a proxy for discrete state models.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{1HTM}$
\end_inset


\end_layout

\begin_layout Standard
The model is a extended version of the general form described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Discrete-State-models"

\end_inset

.
 The extension accounts for the experimental factors intended to influence
 specific processes.
 The most general form, herein after referred to as 
\begin_inset Formula $\text{1HTM}$
\end_inset

, has parameter denoted as 
\begin_inset Formula $d_{\text{weak}}$
\end_inset

 and 
\begin_inset Formula $d_{\text{strong}}$
\end_inset

 for weak and strong encoded items, respectively.
 Additionally it has two full parameter sets for each Scale denoted by the
 index low and high for low-risk and high-risk scales, respectively.
 Let 
\begin_inset Formula $E$
\end_inset

 denote the number of encoding strengths without the zero repetition and
 
\begin_inset Formula $Z$
\end_inset

 the number of scales, the probability of observing a rating 
\begin_inset Formula $m$
\end_inset

 from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure Left
\emph default
) to 
\begin_inset Formula $m=M$
\end_inset

(
\emph on
Sure right
\emph default
) for any given factor combination
\begin_inset Formula $(\epsilon,\zeta)_{1\leq\epsilon\leq E,\,1\leq\xi\leq Z}$
\end_inset

 and left-old, right-old an no-old trials, respectively, are given as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\text{left},\epsilon,\zeta) & = & d_{\epsilon,\zeta}\delta_{l,m,\zeta}+(1-d_{\epsilon,\zeta})\gamma_{m,\zeta}\\
P(m\vert\text{rigth},\epsilon,\zeta) & = & d_{\epsilon,\zeta}\delta_{l,M-m+1,\zeta}+(1-d_{\epsilon,\zeta})\gamma_{m,\zeta}\\
P(m\vert\text{none},0,\zeta) & = & \gamma_{m,\zeta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The model's degree of freedom are given as 
\begin_inset Formula $Z\left(E+M-1\right)$
\end_inset

 which evaluates to 18, 9, 14, 7 for Experiment 1 Session 1, 2 and Experiment
 2 Session 1, 2, respectively.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset


\end_layout

\begin_layout Standard
This model is derived from the 1HTM via setting 
\begin_inset Formula $d_{\text{strong}}=d_{\text{weak}}$
\end_inset

.
 If this restriction holds the experimental manipulation of the encoding
 strength had no influence on the probability of entering a certainty state.
 As increased memory performance is assumed for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
stronger encoded items, asserting this model would question either the validity
 of the models assumptions regarding the memory process or the effectivity
 of the experimental manipulation.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang american
The model's degree of freedom are given as 
\begin_inset Formula $ZM$
\end_inset

 which evaluates to 16, 8, 12, 6 for Experiment 1 Session 1, 2 and Experiment
 2 Session 1, 2, respectively.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 
\end_layout

\begin_layout Standard
This model is derived from the 1HTM via setting 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $d_{\text{low}}=d_{\text{high}}$
\end_inset

.
 If this restriction holds the experimental manipulation of the response
 scale had no influence on the probability of entering a certainty state.
 As different response scales are assumed to influence only the state-response
 mapping and not the probability of entering the confidence state, this
 model is expected to hold.
 If this model will be rejected the validity of the models assumption are
 to be questioned, as it is not reasonable that manipulation of the response
 scale influences memory performance.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang american
The model's degree of freedom are given as 
\begin_inset Formula $E+Z(M-1)$
\end_inset

 which evaluates to 16, 12 for the first sessions of Experiment 1 and 2,
 respectively.
 For the second sessions without scale manipulation this model is equivalent
 to the 1HTM and therefor disregarded.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}},\,\delta,\gamma_{\text{low}}=\delta,\gamma_{\text{high}}}$
\end_inset


\end_layout

\begin_layout Standard
This model is derived from the 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 via setting 
\begin_inset Formula $\delta_{\text{low}}=\delta_{\text{low}}$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\gamma_{\text{low}}=\gamma_{\text{high}}$
\end_inset

.
 If these restrictions hold manipulation of the response scale hat no influence
 on the state-response mapping parameters.
 As response mapping is assumed to be dependent on the response scale, asserting
 this model would question either the validity of the models assumption
 regarding the state-response mapping or the effectivity of the experimental
 manipulation.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang american
The model's degree of freedom are given as 
\begin_inset Formula $E+M-1$
\end_inset

 which evaluates to 9, 7, for the first sessions of Experiment 1 and 2,
 respectively..
 For the second sessions without scale manipulation this model is equivalent
 to the 1HTM and therefor disregarded.
\end_layout

\begin_layout Subsubsection
Continuous models
\end_layout

\begin_layout Standard
In this thesis I focus on the EVSDT as a proxy for continuous models.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{EVSDT}$
\end_inset


\end_layout

\begin_layout Standard
The model is a extended version of the general form described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Latent-Strength-models"

\end_inset

.
 The extension accounts for the experimental factors indented to influence
 specific processes.
 The most general form, herein after referred to as EVSDT, has parameters
 denoted as 
\begin_inset Formula $\mu_{\text{weak}}$
\end_inset

 and 
\begin_inset Formula $\mu_{\text{strong}}$
\end_inset

 for weak and strong encoded items, respectively.
 Additionally it has two full parameter sets for each Scale denoted by the
 index low and high for low-risk and high-risk scales, respectively.
 For any factor combination 
\begin_inset Formula $\epsilon,\zeta$
\end_inset

 the probability of response 
\begin_inset Formula $m$
\end_inset

 is given for left-old, right-old and no-old trials, respectively:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\text{left},\epsilon,\zeta) & = & \Phi\left(\frac{c_{m,\zeta}+\mu_{\epsilon,\zeta}}{\sqrt{2}}\right)-\Phi\left(\frac{c_{m-1,\zeta}+\mu_{\epsilon,\zeta}}{\sqrt{2}}\right)\\
P(m\vert\text{rigth},\epsilon,\zeta) & = & \Phi\left(\frac{c_{m,\zeta}-\mu_{\epsilon,\zeta}}{\sqrt{2}}\right)-\Phi\left(\frac{c_{m-1,\zeta}-\mu_{\epsilon,\zeta}}{\sqrt{2}}\right)\\
P(m\vert\text{none},0,\zeta) & = & \Phi\left(\frac{c_{m,\zeta}}{\sqrt{2}}\right)-\Phi\left(\frac{c_{m-1,\zeta}}{\sqrt{2}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The model's degree of freedom are equal to the corresponding 1HTM model
 and given as 
\begin_inset Formula $Z\left(E+M-1\right)$
\end_inset

.
 That is 18, 9, 14, 7 df for Experiment 1 Session 1, 2 and Experiment 2
 Session 1, 2, respectively.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{strong}}=\mu_{\text{weak}}}$
\end_inset

 
\end_layout

\begin_layout Standard
This model is derived from the EVSDT via setting 
\begin_inset Formula $\mu_{\text{strong}}=\mu_{\text{weak}}$
\end_inset

.
 If this restriction holds different encoding strengths did not lead to
 a different shifts of the familiarity distribution.
 As with the 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

, asserting this model would question it's validity or the effectiveness
 of the experimental manipulation.
 The model's degree of freedom are given, equivalently to the corresponding
 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

, as 
\begin_inset Formula $ZM$
\end_inset

 which evaluates to 16, 8, 12, 6 for Experiment 1 Session 1, 2 and Experiment
 2 Session 1, 2, respectively.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 
\end_layout

\begin_layout Standard
This model is derived from the EVSDT via setting 
\begin_inset Formula $\mu_{\text{low}}=\mu_{\text{high}}$
\end_inset

.
 If this restriction holds different response scales did not lead to a different
 shifts of the familiarity distributions.
 This is, equivalently to the 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

, expected, as it is not reasonable to assume that a different scale affects
 the familiarity of the presented words
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Although, and in contrast to the 1HTM, it could be discussed wether framing
 influences familiarity by paving or inhibiting associations.
 Nevertheless, finding these effects would seriously weaken the validity
 of 
\begin_inset Formula $\mu$
\end_inset

 as a memory parameter.
 
\end_layout

\end_inset

 and a failure to reject this model model would cast doubt on the validity
 of 
\begin_inset Formula $\mu$
\end_inset

 as a memory parameter.
 The model's degree of freedom are given, equivalently to the corresponding
 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 , as 
\begin_inset Formula $E+Z(M-1)$
\end_inset

 which evaluates to 16, 12 for the first sessions of Experiment 1 and 2,
 respectively.
 For the second sessions without scale manipulation this model is equivalent
 to the EVSDT and therefor disregarded.
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}},\, c_{\text{low}}=c_{\text{high}}}$
\end_inset


\end_layout

\begin_layout Standard
This model is derived from the 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 via setting all 
\begin_inset Formula $ $
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $c_{i,\text{low}}=c_{i,\text{high}}$
\end_inset

 for all 
\begin_inset Formula $0\leq i\leq M$
\end_inset

.
 If this restriction holds different response scales did not lead to different
 cutoff criteria.
 As these cutoff criteria are assumed to be dependent on the response scales,
 asserting this model would question either the validity of the models assumptio
ns regarding the response process or the effectivity of the experimental
 manipulation.
 The model's degree of freedom are given, equivalently to the 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang american

\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}},\,\delta,\gamma_{\text{low}}=\delta,\gamma_{\text{high}}}$
\end_inset

, as 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $E+M-1$
\end_inset

 which evaluates to 9, 7, for the first sessions of Experiment 1 and 2,
 respectively..
 For the second sessions without scale manipulation this model is equivalent
 to the 1HTM and therefor disregarded.
\end_layout

\begin_layout Subsubsection
Continuous
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The observed frequency for the confidence levels under each unique factor
 combination is calculated.
 The obtained category frequencies are then fit in R 
\begin_inset CommandInset citation
LatexCommand citep
key "r_core_team_r:_2013"

\end_inset

 using the package MPTinR
\begin_inset CommandInset citation
LatexCommand citep
key "singmann_mptinr:_2013"

\end_inset

 to the unrestricted and various restricted versions of the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 and the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

.
 The fitting algorithm used by MPTinR is a minimization of the log-likelihood
 function of the to be fitted model performed by the R-function
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
nlminb
\end_layout

\end_inset

 that uses the PORT optimization routine 
\begin_inset CommandInset citation
LatexCommand citep
key "gay_usage_1990"

\end_inset

.
 Obtained fits are evaluated in terms of 
\begin_inset Formula $G^{2}$
\end_inset

-values because of several advantages over the log-likelihood values.
 The most important being 
\begin_inset Formula $G^{2}$
\end_inset

-values, sums of 
\begin_inset Formula $G^{2}$
\end_inset

 values of the same model fit to different individuals, and differences
 of 
\begin_inset Formula $G^{2}$
\end_inset

-values of restricted versions of the same model, all being 
\begin_inset Formula $\chi^{2}$
\end_inset

-distributed with dfs being the data's df minus the number of estimated
 parameters, the sum of all individual dfs, and the gain in df achieved
 by restricting the model, respectively 
\begin_inset CommandInset citation
LatexCommand citep
key "riefer_multinomial_1988"

\end_inset

.
 This allows testing for significant deviations from the observed data for
 all participants at once and individually, and both also for nested models
 against their unrestricted versions.
 It is noteworthy that the test for summed data is very powerful and it
 is important to check if a rejection holds also for the majority of the
 participants to decrease risk of an 
\begin_inset Formula $\alpha$
\end_inset

- failure.
 Speaking of which, 
\begin_inset Formula $\alpha$
\end_inset

-level is set to be .05 for all tests conducted.
\end_layout

\begin_layout Chunk

<<DataSetup, cache=TRUE, echo=FALSE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
First the effects of the experimental manipulations are presented for each
 experiment individually.
 Then the equivalent models (plus scale manipulation) to the ones used by
 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 are compared.
 Lastly the stability of each participants superior model across words and
 pictures as memorized items is presented.
\end_layout

\begin_layout Subsection
Experiment 1
\end_layout

\begin_layout Chunk

<<LoadExp1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The most general MPT1HTM model provided a good account for both sessions
 and summed across all participants 
\begin_inset Formula $G^{2}$
\end_inset

(
\end_layout

\begin_layout Subsubsection
Effects of manipulation
\end_layout

\begin_layout Chunk

<<Res01, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
In this experiment repetition of items in the learning phase was varied
 in both sessions.
 In session one additionally two response scales were used.
 In the first session the memorized items were word and in the second pictures.
 First we check if different repetitions influenced memory...
\end_layout

\begin_layout Paragraph
Encoding strength
\end_layout

\begin_layout Standard
The restricted 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

 with equal memory parameter both encoding strengths failed to account for
 all data sets and has been be rejected on basis of the summed indices 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session 2, respectively.
 This model has also been rejected on individual level for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants, in Session 1 and 2, respectively.
 It can safely be concluded that encoding strength manipulation has been
 reflected in the memory parameters of the 
\begin_inset Formula $\text{1HTM}$
\end_inset

.
 The same holds true, yet even clearer, for the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

.
 The restricted 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{strong}}=\mu_{\text{weak}}}$
\end_inset

 was rejected to account well for the data 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session2, respectively.
 Like the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 it was rejected for most (i.e.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

, in Session 1, and 2, respectively) participants.
 Thus, 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{strong}}=\mu_{\text{weak}}}$
\end_inset

 have been excluded from further analysis.
\end_layout

\begin_layout Standard
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}},\, g\gamma_{\text{low}}=g\gamma_{\text{high}}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}},\, c_{\text{low}}=c_{\text{high}}}$
\end_inset


\end_layout

\begin_layout Paragraph
Scale
\end_layout

\begin_layout Standard
The models with memory parameter restricted to be equal across the different
 scales used in Session 1 were more parsimonious without reducing fit 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 The null that both models are equal in terms of GoF holds also on individual
 level with 
\emph on
p 
\emph default
< .05 only for
\emph on
 
\emph default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[2]]}
\end_layout

\end_inset

 participants out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Based on these findings further analysis is done using the more parsimonious
 versions of MPT1HTM and EVSDT with memory parameter restricted to be equal
 across scales.
\end_layout

\begin_layout Standard
To check wether the scale manipulation has any influence at all the MPT1HTM
 and EVSDT models are further restricted to have their mapping parameter
 and cutoff criteria, respectively, restricted to be equal across the different
 scales.
 These fully restricted versions which assume no influence of the scale
 manipulation had to be rejected: 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Individually these models were rejected at 
\emph on
p
\emph default
 < .05 for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[2]]}
\end_layout

\end_inset

 participants, for MPT1HTM and EVSDT, respectively.
\end_layout

\begin_layout Subsubsection
Model comparison
\end_layout

\begin_layout Chunk

<<Res02, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The restricted versions of MPT1HTM and EVSDT with the best and parsimonious
 account for the data (i.e.
 the versions with only memory parameters restricted to be equal across
 scales), are compared against each other.
 Since both models have equal number of parameters (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

 for Session 1, 2 respectively) classical selection criteria like AIC and
 BIC are merely a function of fit, hence the 
\begin_inset Formula $G^{2}$
\end_inset

-statistics are used directly.
 The differences are not conclusive (
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Median"], digits=2)}
\end_layout

\end_inset

, range: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Max."], digits=2)}
\end_layout

\end_inset

, and 
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Median"], digits=2)}
\end_layout

\end_inset

, range:
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Max."], digits=2)}
\end_layout

\end_inset

, for Session 1 and 2, respectively, greater values imply preference for
 MPT1HTM).
 Individually accounted for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses1[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses2[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants in Session 1 and 2, respectively, the MPT1HTM model better
 for the data.
 This further supports the overall impression that the models in this experiment
 performed approximately equally well.
\end_layout

\begin_layout Chunk

<<Res02p1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<Res02p2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsubsection
Stability of individual best model
\end_layout

\begin_layout Chunk

<<Res03, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
Session 1 and Session 2 has been completed by 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.both.ses"]}
\end_layout

\end_inset

 participants.
 In each session every participant's data was best captured with either
 the MPT1HTM or the EVSDT model.
 Across session this best model was stable for 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.stable"]}
\end_layout

\end_inset

 participants and thus just about the randomly expected rate of 50%.
\end_layout

\begin_layout Subsection
Experiment 2
\end_layout

\begin_layout Chunk

<<LoadExp2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The most general MPT1HTM model provided a good account for both sessions
 and summed across all participants 
\begin_inset Formula $G^{2}$
\end_inset

(
\end_layout

\begin_layout Subsubsection
Effects of manipulation
\end_layout

\begin_layout Chunk

<<Res01, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The key manipulation of this experiment was the addition of trials in which
 both items have not been memorized.
 Additionally, 
\begin_inset Formula $M=6$
\end_inset

 for all rating scales in this experiment.
 Beside this the experiment was similar to Experiment 1: repetition of items
 in the learning phase was varied in both sessions and in session one two
 response scales were used.
\end_layout

\begin_layout Paragraph
Encoding strength
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

 could be rejected 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session 2, respectively.
 This model was also individually rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants, in Session 1 and 2, respectively, with p < .05, as well in
 Experiment 1 this rejection is not based on special cases and it can be
 concluded, that encoding strength affects memory parameters.
 The same pattern, again even clearer, is found for the 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{strong}}=\mu_{\text{weak}}}$
\end_inset

 with 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session2, respectively.
 Individually it was rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 participants, in Session 1 and Session 2, respectively.
 As in Experiment 1 both 
\begin_inset Formula $\text{1HTM}_{d_{\text{strong}}=d_{\text{weak}}}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{strong}}=\mu_{\text{weak}}}$
\end_inset

 are excluded from further analysis.
\end_layout

\begin_layout Paragraph
Scale
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 was more parsimonious without reducing fit 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

.
 This is further supported on individual level only for
\emph on
 
\emph default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[2]]}
\end_layout

\end_inset

 participants the model was rejected at 
\emph on

\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\alpha$
\end_inset

 
\emph default
< .05.
 The 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 in contrast differed from the unrestricted 
\begin_inset Formula $\text{EVSDT}$
\end_inset

 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 the null that both models are equal in terms of GoF had to be rejected.
 But this is most likely to the high power of the 
\begin_inset Formula $G^{2}$
\end_inset

-test for summed data - as the test for individual participants only rejected
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[2]]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

 - and the general bad fit of the EVSDT model for the data in this experiment.
 Despite this result for summed fit indices of the 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 in order to stay close to 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 both more parsimonious models 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 are used for further analysis
\begin_inset Foot
status open

\begin_layout Plain Layout
Results should be tested with the additional df for EVSDT
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Successful scale manipulation in Session one could be confirmed for both
 the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}$
\end_inset

.
 The most restricted versions 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}},\, g\gamma_{\text{low}}=g\gamma_{\text{high}}}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}},\, c_{\text{low}}=c_{\text{high}}}$
\end_inset

, which assume no influence of the scale manipulation, whatsoever, had to
 be rejected: 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for 
\begin_inset Formula $\text{1HTM}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}$
\end_inset

, respectively.
 Individually these models were rejected at
\emph on
 
\begin_inset Formula $\alpha$
\end_inset


\emph default
 < .05 for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[2]]}
\end_layout

\end_inset

 participants, for 
\begin_inset Formula $\text{1HTM}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}$
\end_inset

, respectively.
\end_layout

\begin_layout Subsubsection
Model comparison
\end_layout

\begin_layout Chunk

<<Res02, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
For the final comparison the same models as 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 compared are used, namely 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

 and 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 (i.e.
 the versions with only memory parameters restricted to be equal across
 scales).
 Both models have again equal number of parameters (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

 for Session 1, 2 respectively) and the 
\begin_inset Formula $G^{2}$
\end_inset

-statistics are used.
 The differences are clearly in favor of the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 (
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Median"], digits=2)}
\end_layout

\end_inset

, range: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Max."], digits=2)}
\end_layout

\end_inset

, and 
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Median"], digits=2)}
\end_layout

\end_inset

, range:
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Max."], digits=2)}
\end_layout

\end_inset

, for Session 1 and 2, respectively, greater values imply preference for
 the 
\begin_inset Formula $\text{1HTM}$
\end_inset

).
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:g2-values-exp1"

\end_inset

 illustrated the these finding and gives an overview for the general pattern8.
 The individual comparisons further underpin the advantage of the 
\begin_inset Formula $\text{1HTM}$
\end_inset

 over the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses1[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses2[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants in Session 1 and 2, respectively, are better described by
 the 
\begin_inset Formula $\text{1HTM}$
\end_inset

.
 The pattern found by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

, thus, has clearly been confirmed.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Formula $G^{2}$
\end_inset

-values for 
\begin_inset Formula $\text{EVSDT}_{\mu_{\text{low}}=\mu_{\text{high}}}$
\end_inset

 as function of the same participant's 
\begin_inset Formula $G^{2}$
\end_inset

-values for 
\begin_inset Formula $\text{1HTM}_{d_{\text{low}}=d_{\text{high}}}$
\end_inset

.
 Grey line shows equal values and black line is a liner least-squared deviation
 fitted line, visualizing the general tendency for higher values.
 The most discriminating participants are labeled.
\begin_inset CommandInset label
LatexCommand label
name "fig:g2-values-exp1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chunk

<<Res02p1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

\end_layout

\end_inset


\end_layout

\begin_layout Chunk

<<Res02p2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsubsection
Stability of individual best model
\end_layout

\begin_layout Chunk

<<Res03, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
Session 1 and Session 2 has been completed by 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.both.ses"]}
\end_layout

\end_inset

 participants.
 In each session every participant's data was best captured with either
 the MPT1HTM or the EVSDT model.
 Across session this best model was stable for 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.stable"]}
\end_layout

\end_inset

 participants and thus just about the randomly expected rate of 50%.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
As expected manipulation of encoding strength and response-scale manipulation
 in both sessions and Session 1, respectively, influenced the expected and
 only the expected parameter for the majority of participants and the summed
 data in both experiments.
 In the first Experiment, without zero repetition condition, the comparison
 between discrete-state and continuous model was inconclusive.
 Evidently, the inclusion of the zero repetition condition in Experiment
 2 led to the clear advantage of the discrete-state model.
 This result replicates the pattern found by 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 and stresses the relevance of the zero repetition condition.
 Thus, all hypotheses have been confirmed.
 The remaining part of this thesis I will discuss the conclusions for the
 use of discrete-state models in recognition memory that can be drawn upon
 these results and give suggestions for further research.
\end_layout

\begin_layout Subsection
Relevance for Recognition Memory Research
\end_layout

\begin_layout Standard
The two experiments provides a solid base for the conclusion that the inclusion
 of zero repetition trials, that is trials in that the experimenter can
 be sure no memory processes have been involved in the constitution of the
 observable response, is crucial to discriminate between discrete-state
 and continuous models of recognition memory.
 But is the advantage for discrete state models due to a lack of a shifted
 unimodal distribution for weakly encoded items as suggested by 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

? Whereas there is no reason to deny this, 
\end_layout

\begin_layout Standard
In an experiment without memory free condition like Experiment 1 in this
 thesis, the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

 accommodates by a sufficient shift of the familiarity distribution (i.e.
 greater 
\begin_inset Formula $\mu$
\end_inset

) and therefore able to achieve comparable fit even with different memory
 strengths.
 Only the zero repetition condition impedes this by restricting the cutoff
 criteria as there are no other parameter to account for the guessing distributi
on.
 This restriction holds equivalently for the 
\begin_inset Formula $\text{1HTM}$
\end_inset

's 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 parameters.
 Every change in these parameters directly reduces fit for the guessing
 distribution.
 As the guessing distribution in real world situation is mostly fairly symmetric
 and the here used 
\begin_inset Formula $\text{1HTM}$
\end_inset

 correctly accounts for this by assuming a symmetric mapping 
\begin_inset Formula $\gamma_{l}=\gamma_{r}$
\end_inset

 possibly to unequal parts determined by 
\begin_inset Formula $g$
\end_inset

 to account for left-right tendencies.
 However the 
\begin_inset Formula $\text{EVSDT}$
\end_inset

 is free to fit all possible distributions.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\text{EVSDT}$
\end_inset

 is very flexible in accounting for manipulations of the the overall frequency
 in each response category (insignificant wether obtained via base-rate
 manipulation, payoff conditions, or confidence ratings)
\end_layout

\begin_layout Standard
Zero repetition
\end_layout

\begin_layout Standard
Experimental situation advantagous for MPT 
\end_layout

\begin_layout Standard
More parameter models, but what should these parameter captue?
\end_layout

\begin_layout Standard
Strong memories are hard to scale 
\begin_inset CommandInset citation
LatexCommand citep
key "mickes_strong_2011"

\end_inset


\end_layout

\begin_layout Subsection
Approach to Further Research
\end_layout

\begin_layout Standard
Varying the amount of non-showns.
 
\end_layout

\begin_layout Standard
Reasonable to assume that recognition is very clear like argued by Mickes
 et al.
 most importantly one has to decide wether yes or no.
 It doesn't equip people for their everyday work if they have can flexibly
 assign probabilities to memory.
 -> Introductory example
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "MasterThesis"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Appendices
\end_layout

\begin_layout Subsection
Fit Table
\end_layout

\begin_layout Standard
j
\end_layout

\begin_layout Subsection
Participants' plots
\end_layout

\begin_layout Chunk

<<AllPlots, eval=TRUE, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=21*0.75,
 fig.height=31*0.75>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Data Manipulation R-code
\end_layout

\begin_layout Chunk

<<DataManipulation.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<DataManipulation>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Model Fitting R-code
\end_layout

\begin_layout Chunk

<<ModelFitting.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<ModelFitting>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Model specifications R-code
\end_layout

\begin_layout Chunk

<<ModelSpecifications.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<ModelSpecifications>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Plotting R-code
\end_layout

\begin_layout Chunk

<<Plotting.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<Plotting>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

\end_layout

\end_body
\end_document
