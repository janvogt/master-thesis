#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass apa6
\begin_preamble
\usepackage[american]{babel}	
\usepackage{csquotes}
\usepackage[style=apa,natbib=true,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{/Users/jan/Studium/Masterarbeit/Thesis/MasterThesis.bib}
\AtBeginDocument{\renewcommand{\ref}[1]{\mbox{\autoref{#1}}}}
\end_preamble
\use_default_options true
\begin_modules
knitr
biblatex
logicalmkup
\end_modules
\maintain_unincluded_children false
\language american
\language_package none
\inputencoding utf8
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout ShortTitle
Recognition Memory
\end_layout

\begin_layout Title
Disentangling Cognitive Processes with Mathematical Modeling:
\emph on
 Evaluating Continuous and Discrete-State Models of Recognition Memory via
 Response-Scale and Encoding-Strength Manipulation
\end_layout

\begin_layout Author
Jan Vogt
\end_layout

\begin_layout Abstract
Abstract
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chunk

<<includes, echo=FALSE, eval=TRUE, chache=FALSE>>=
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/setup.R')
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/datahandling.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/fitting.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/modelspec.R') 
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/plotting.R')
\end_layout

\begin_layout Chunk

	read_chunk('Rscripts/chunks.R')
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<setup, echo=FALSE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<DataManipulation, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<ModelFitting, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<ModelSpecifications, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<Plotting, eval=TRUE, echo=FALSE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Who has not experienced it: a perceived stranger approaches you with a warm
 smile and asks how are you as if you well known to each other.
 As awkward these situations are in real life as interesting is the research
 on the underlying process - recognition memory.
 Depending on your confidence in your recognition memory abilities, you
 might assume the foreigner to falsely recognize you or try to frantically
 remember where you had met this person and who it is.
 In most of these situations the truth will stay undetermined, but it is
 for sure, that for each role there are two possibilities.
 The person recognizing you can do that correctly, in recognition memory
 research this is called a 
\emph on
hit
\emph default
, or mistakenly, this is called a 
\emph on
false alert
\emph default
.
 You on the other hand not recognizing the person can be because it is a
 stranger, this is called 
\emph on
correct rejection
\emph default
, or you could be wrongly not recognize a known person, this is called a
 
\emph on
miss
\emph default
.
 But does the person recognizing you have a superior recognition memory
 ability or are you just more conservative in deciding wether you know one
 or not? Without any theory about the underlying processes this question
 can not be answered.
\end_layout

\begin_layout Standard
To generalize from this introductory example consider the other person from
 both peoples point of view as any item that could be known or not.
 Hit and miss translate identifying an known item as old or new, respectively
 and correct rejection and false alert translate to identifying an unknown
 item as new or old, respectively.
 The statistically experienced reader might easily see the fourfold pattern
 of two item types (known and unknown) and two response options (old and
 new) and conclude to answer the problem memory ability versus response
 bias via the overall probability to answer with old being the response
 bias and the odds ratio of answering old for known items over answering
 old for unknown ones as the memory ability.
 But although this would lead to an answer it is obvious that the parameter
 do not fulfill reasonable assumptions about the two processes.
 For example it it does not make sense that memory is dependent on response
 bias, but in this model it would be: Consider someone with perfect memory
 that is being able to remember everything.
 This person would identify every known item as known.
 But despite perfect memory the proposed odds ratio as memory indicator
 would vary based on that person's tendency to falsely identify unknown
 items as old, that is the memory indicator would increase as much as the
 person tends to identify items as unknown in general.
 A good memory indicator should identify the actual memory performance without
 being influenced by the individual preference for one or the other answer.
\end_layout

\begin_layout Standard
Mathematical modeling aims to disentangle these processes and by describing
 the underlying processes in a formal predictable fashion that allows to
 select competing models based on quantitative indicators, measure the influence
 of experimental treatments on specific subprocesses, and predict models
 idiosyncratic data patterns.
 This thesis uses all these abilities to compare two competing model classes
 for their ability to account for observed recognition memory data.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Standard
In the literature several models of recognition memory are discussed
\begin_inset CommandInset citation
LatexCommand citep
after "for a recent overview"
before "see"
key "snodgrass_pragmatics_1988,klauer_flexibility_2011"

\end_inset

.
 The most prominent ones are models based on the theory of signal detection
 
\begin_inset CommandInset citation
LatexCommand citep
key "green_signal_1966"

\end_inset

 and high threshold models 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g. "
key "blackwell_neural_1963,snodgrass_pragmatics_1988"

\end_inset

.
 The main difference of these models is the assumption underlying the nature
 of the memory process.
 However, both models inherently distinguish two types of processes, memory
 processes and response processes.
 That is, it is generally agreed upon that memory researchers need models
 to measure the quantities of interest.
 
\end_layout

\begin_layout Standard
Results of experiments involving a finite set of behavioral categories 
\begin_inset Formula $C_{1},C_{2},\ldots,C_{J}$
\end_inset

 can be described exhaustively as a set of each categories' observed frequency
 
\begin_inset Formula $D=\{N_{1},N_{2},\ldots,N_{j}\}$
\end_inset

.
 Under the assumption that these frequencies are mutually independent and
 identically distributed there is a probability 
\begin_inset Formula $p_{j}$
\end_inset

 for observing category 
\begin_inset Formula $C_{j}$
\end_inset

.
 The probability for finding a dataset under a given set of probabilities
 for each category is given as 
\begin_inset Formula $P(D\vert p_{1},p_{2},\ldots,p_{J})=\sum_{i=1}^{J}(N_{i})!\prod_{i=1}^{J}\frac{p_{i}^{N_{i}}}{N_{i}!}$
\end_inset

.
 Psychologists are usually interested in the generating cognitive processes
 of the observed data pattern 
\begin_inset Formula $D$
\end_inset

.
 Often the assumed processes and their relationships are described verbally,
 appropriate experimental manipulations are applied and corresponding changes
 in 
\begin_inset Formula $D$
\end_inset

 are hypothesized.
 If the null hypothesis is rejected, i.e.
 the probability for the found data in the experimental group under the
 assumption of equal probabilities for the categories to the control group
 is smaller than an acceptable error level 
\begin_inset Formula $P(D_{EG}\vert p_{1,}p_{2},\ldots,p_{J})<\alpha$
\end_inset

, the described process is seen as confirmed.
 Problems, which arises using this approach, are that as language is rather
 flexible it is often possible to explain unexpected results within the
 same processes and that interacting parts of the assumed processes can
 be difficult to link to the observable results.
 A possible solution to these problems is the use of a formal description
 of the assumed processes, i.e.
 a mathematical model linking the influence of an experimental manipulation
 to the observable data 
\begin_inset Formula $D$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Latent-Strength models
\end_layout

\begin_layout Standard
Latent strength models assume that all stimuli vary along a familiarity
 scale.
 Its is usually assumed (although this is to some extent arbitrary) that
 the familiarity of items follows a normal distribution.
 When a set of items is presented to an individual during a study phase,
 it is assumed that this leads to an increase of familiarity.
 This familiarity increase for the studied items is represented by a distributio
n that assumes larger values on average (it is shifted to the right) than
 the distribution of non-studied items.
 Responses are made based on a cutoff criterion, that is items with familiarity
 above this criteria are classified as known and items below as unknown.
 Memory strength is captured as the amount the distribution of studied items
 is shifted.
 Response tendency is captured in the position of the cutoff criterion -
 the higher the cutoff criteria the lower the probability for classifying
 items as known (i.e.
 the more conservative the response bias).
 The most general form of this model has a parameter set 
\begin_inset Formula $\theta=\{\mu_{o},\mu_{n},\sigma_{o},\sigma_{n},c\}$
\end_inset

 with 
\begin_inset Formula $\{\mu_{o},\sigma_{o}\}$
\end_inset

 and 
\begin_inset Formula $\{\mu_{n},\sigma_{n}\}$
\end_inset

being the mean and standard deviation of old and new items, respectively,
 and 
\begin_inset Formula $c$
\end_inset

 being the response criterion.
 Without loss of generality, the mean and standard deviation of unknown
 items can be set to 0 and 1, respectively.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\Phi(x)$
\end_inset

 denote the cumulative distribution function of the standard normal distribution
, the probability for a hit and a false alert are then given as
\begin_inset Formula 
\begin{align*}
P(\text{"old"}\vert\theta,\text{old item}) & =\Phi\left(\frac{\mu_{o}-c}{\sigma_{o}}\right)\\
P(\text{"old"}\vert\theta,\text{new item}) & =\Phi(-c)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Responses to 
\begin_inset Formula $M$
\end_inset

-step bipolar confidence rating scales (
\begin_inset Formula $M$
\end_inset

 being an even integer), going from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure New
\emph default
) and 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Old
\emph default
), are assumed to be based on 
\begin_inset Formula $M+1$
\end_inset

 ordered cutoff criteria.
 The models parameter are then given as 
\begin_inset Formula $\theta=\{\mu,\sigma,c_{0},c_{1},\ldots,c_{M}\}$
\end_inset

, with 
\begin_inset Formula $c_{0}=-\infty<c_{1}<...<c_{M}=+\infty$
\end_inset

 .
 Let 
\begin_inset Formula $m$
\end_inset

 be the observed confidence rating (
\begin_inset Formula $1\leq m\leq M$
\end_inset

).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to an old and new item are respectively 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{old item}) & = & \Phi\left(\frac{c_{m}-\mu}{\sigma}\right)-\Phi\left(\frac{c_{m-1}-\mu}{\sigma}\right)\\
P(m\vert\theta,\text{new item}) & = & \Phi\left(c_{m}\right)-\Phi\left(c_{m-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In a two-alternative forced-choice (2AFC) task two items are presented on
 each trial (e.g., one item on the left and the other on the right side),
 and the individual's task is to indicate which of the two corresponds to
 the old item.
 It is assumed that individuals choose the item with the largest familiarity
 value.
 The model can conceptualized in terms of distributions representing the
 difference in familiarity between the two items, let us say the difference
 between the item on the left and the item on the right.
 When the old item is on the left, the familiarity difference follows a
 normal distribution with mean 
\begin_inset Formula $\mu$
\end_inset

 and standard deviation
\begin_inset Formula $\sqrt{1+\sigma^{2}}$
\end_inset

.
 When the old item is on the right, the familiarity difference follows a
 normal distribution with mean 
\begin_inset Formula $-\mu$
\end_inset

 and standard deviation 
\begin_inset Formula $\sqrt{1+\sigma^{2}}$
\end_inset

.
 We could also consider a case in which none of the items is old, a case
 in which the difference distribution has mean 0 and standard deviation
 of 
\begin_inset Formula $\sqrt{2}$
\end_inset

.
 Now, let us consider a 
\begin_inset Formula $M$
\end_inset

-step bipolar confidence rating scales going from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure Left
\emph default
) to 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Right
\emph default
).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to a left-old and right-old item are respectively 
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{left}) & = & \Phi\left(\frac{c_{m}-\mu}{\sqrt{1+\sigma^{2}}}\right)-\Phi\left(\frac{c_{m-1}-\mu}{\sqrt{1+\sigma^{2}}}\right)\\
P(m\vert\theta,\text{rigth}) & = & \Phi\left(\frac{c_{m}+\mu}{\sqrt{1+\sigma^{2}}}\right)-\Phi\left(\frac{c_{m-1}+\mu}{\sqrt{1+\sigma^{2}}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In this thesis I will focus on the equal-variance SDT model, in which
\begin_inset Formula $\sigma_{o}$
\end_inset

 is also fixed to 1.
\end_layout

\begin_layout Subsection
Discrete-State models
\end_layout

\begin_layout Standard
Discrete-state models assume that observed responses result from a mixture
 of discrete mental states which are entered or not depending on the probabilist
ic occurrence of specific cognitive processes.
 In a old-new recognition task, it is assumed that old items either enter
 a certainty state (
\begin_inset Formula $S_{1}$
\end_inset

) with probability 
\begin_inset Formula $d_{o}$
\end_inset

 or an uncertainty (
\begin_inset Formula $S_{2}$
\end_inset

) state with probability 
\begin_inset Formula $1-d_{o}$
\end_inset

.
 When in state 
\begin_inset Formula $S_{1}$
\end_inset

, the old items are invariably recognized (response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

).
 New items enter certainty state 
\begin_inset Formula $S_{3}$
\end_inset

 with probability 
\begin_inset Formula $d_{n}$
\end_inset

 or the uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

 with probability 
\begin_inset Formula $1-d_{n}$
\end_inset

.
 New items in state 
\begin_inset Formula $S_{3}$
\end_inset

 are invariably rejected (response 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

).
 When items enter the uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

, their status cannot be ascertained and hence a guessing process ensues.
 Response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 is given with probability 
\begin_inset Formula $g$
\end_inset

 and response 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

 is given with probability 
\begin_inset Formula $1-g$
\end_inset

.
 The probability of response 
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 for old and new items is given respectively by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(\text{"old"}\vert\theta,\text{old item}) & = & d_{o}+(1-d_{o})g\\
P(\text{"old"}\vert\theta,\text{new item}) & = & (1-d_{n})g
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The extension of this so called 2 high-threshold model (2HTM) to confidence-rati
ng scales requires the specification of state-response mapping functions
 that assign the probability of each rating response, conditional on the
 discrete state.
 Let 
\begin_inset Formula $\delta_{o}$
\end_inset

, 
\begin_inset Formula $\delta_{n}$
\end_inset

,
\begin_inset Formula $\gamma_{o}$
\end_inset

, and 
\begin_inset Formula $\gamma_{n}$
\end_inset

 be probability vectors with length 
\begin_inset Formula $M$
\end_inset

, each vector summing to 1.
 Vectors 
\begin_inset Formula $\delta_{o}$
\end_inset

 and 
\begin_inset Formula $\delta_{n}$
\end_inset

 are assigned to states 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{3}$
\end_inset

 respectively, and attribute probability 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary response.
 Similarly, vectors 
\begin_inset Formula $\gamma_{o}$
\end_inset

 and 
\begin_inset Formula $\gamma_{n}$
\end_inset

 attribute probability 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary response (
\begin_inset Quotes eld
\end_inset

old
\begin_inset Quotes erd
\end_inset

 with probability 
\begin_inset Formula $g$
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

new
\begin_inset Quotes erd
\end_inset

 with probability 
\begin_inset Formula $1-g$
\end_inset

).
 The probability of response rating 
\begin_inset Formula $m$
\end_inset

 being given to an old and new item are respectively
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{old item}) & = & d_{o}\delta_{o.m}+(1-d_{o})\bigl(g\gamma{}_{o,m}+(1-g)\gamma{}_{n,m}\bigr)\\
P(m\vert\theta,\text{new item}) & = & d_{n}\delta_{n,m}+(1-d_{n})\bigl(g\gamma{}_{o,m}+(1-g)\gamma{}_{n,m}\bigr)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In 2AFC tasks both detection states 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{3}$
\end_inset

 lead invariable to the correct answer (e.g.
 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 independently if the right item is being recognized as the old one or the
 left one as the new).
 Thus 
\begin_inset Formula $d_{o}$
\end_inset

 and 
\begin_inset Formula $d_{n}$
\end_inset

 are not separable and thus replaced by 
\begin_inset Formula $d_{l}$
\end_inset

, and 
\begin_inset Formula $d_{r}$
\end_inset

, representing the probabilities of entering any detect state if the old
 item is presented on the left or right side, respectively.
 Formally they can be understood as 
\begin_inset Formula $d_{l}=1-(1-d_{o})(1-d_{n})$
\end_inset

 if the old item is presented on the left side and 
\begin_inset Formula $d_{r}$
\end_inset

 being the equivalent for the right side.
 The uncertainty state 
\begin_inset Formula $S_{2}$
\end_inset

 leads to guessing wether the old item is left or right with 
\begin_inset Formula $g$
\end_inset

 being the probability for a response 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Formula $1-g$
\end_inset

 for 
\begin_inset Quotes eld
\end_inset

left
\begin_inset Quotes erd
\end_inset

.
 Similar to the yes-no confidence-rating task we define 
\begin_inset Formula $\delta_{l}$
\end_inset

, 
\begin_inset Formula $\delta_{r}$
\end_inset

, 
\begin_inset Formula $\gamma_{l}$
\end_inset

, and 
\begin_inset Formula $\gamma_{r}$
\end_inset

 as the state-response mapping.
 
\begin_inset Formula $\delta_{l}$
\end_inset

 and 
\begin_inset Formula $\delta_{r}$
\end_inset

 are mapping detect states on the left and right side, respectively, to
 rating 
\begin_inset Formula $m$
\end_inset

 with probability 0 for the other side.
 And 
\begin_inset Formula $\gamma_{l}$
\end_inset

, and 
\begin_inset Formula $\gamma_{r}$
\end_inset

 attribute probabilities 0 to all ratings 
\begin_inset Formula $m$
\end_inset

 inconsistent with the binary 
\begin_inset Quotes eld
\end_inset

left
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

right
\begin_inset Quotes erd
\end_inset

 answer, respectively.
 Again, let us consider the 
\begin_inset Formula $M$
\end_inset

-step scale from 
\begin_inset Formula $m=1$
\end_inset

 (
\emph on
Sure left
\emph default
) to 
\begin_inset Formula $m=M$
\end_inset

 (
\emph on
Sure Left
\emph default
),
\emph on
 
\emph default
the probability of response 
\begin_inset Formula $m$
\end_inset

 being given an left-old and right-old item are respectively:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P(m\vert\theta,\text{left}) & = & d_{l}\delta_{l.m}+(1-d_{l})\bigl(g\gamma{}_{r,m}+(1-g)\gamma{}_{l,m}\bigr)\\
P(m\vert\theta,\text{right}) & = & d_{r}\delta_{r,m}+(1-d_{r})\bigl(g\gamma{}_{r,m}+(1-g)\gamma{}_{l,m}\bigr)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In this thesis I will focus on the 1 high-threshold model (1HTM), in which
 there is only one detect parameter for both sides 
\begin_inset Formula $d=d_{l}=d_{r}$
\end_inset

.
\end_layout

\begin_layout Subsection
Model selection
\end_layout

\begin_layout Standard
Consider we have 
\begin_inset Formula $n$
\end_inset

 competing models 
\begin_inset Formula $M=\{m_{1},m_{2},\ldots,m_{n}\}$
\end_inset

, which is the best? To answer this question we need to define what 
\emph on
best
\emph default
 means.
 Whereas it is not trivial to state what is best it is easy to state what
 not.
 If we define the best model as the one that describes the data the closest,
 that is the found data is most likely if this model is true: 
\begin_inset Formula $P(D\vert m_{best})=\max\left(\bigcup_{m\in M}P(D|m)\right)$
\end_inset

 then the best model we could define is the data itself.
 But without any information reduction we would not need any model after
 all.
 Defining the best model as the one which reduces the information the most
 is not reasonable because the perfect case would be to say nothing at all
 about the data, that is we would not have needed even to conducted the
 research if that would be it's goal.
 It becomes clear that 
\emph on
best
\series bold
 
\series default
\emph default
has to be a tradeoff between a models ability to fit data and it's flexibility.
 The much fit as possible with as much flexibility as necessary.
\end_layout

\begin_layout Standard
It follows immediately that if model 
\begin_inset Formula $m_{a}$
\end_inset

 has less flexibility and greater fit than 
\begin_inset Formula $m_{b}$
\end_inset

, the latter can not be the best model.
 But this leaves to open problems: (a) how is flexibility measured in general
 and (b) how is this measure related to fit, which is important to select
 models in cases with trade-offs, for example model 
\begin_inset Formula $m_{a}$
\end_inset

 has better fit but also greater flexibility than 
\begin_inset Formula $m_{b}$
\end_inset

.
 To measure flexibility various methods have been proposed.
\end_layout

\begin_layout Quotation
Some of the principled techniques in use are cross-validation methods, bootstrap
 simulations to assess model mimicry, and the use of model-selection indices
 such as AIC and BIC.
 Model-selection indices are used most frequently in applications of the
 present [recognition memory] models.
 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 431"
key "klauer_flexibility_2011"

\end_inset


\end_layout

\begin_layout Standard
Mathematically it is most reasonable to measure flexibility as the ability
 of a model to account for every possible data pattern.
 This understanding is reflected in indices based on the principle of minimum
 description length (MDL such as the normalized maximum likelihood index
 
\begin_inset CommandInset citation
LatexCommand citep
before "NML;"
key "myung_model_2006"

\end_inset

.
 These indices are used only rarely because they are difficult to compute
 even for modern computers.
 For recognition memory they are just recently applied to binary response
 data
\begin_inset CommandInset citation
LatexCommand citep
key "klauer_flexibility_2011"

\end_inset

but have yet to be extended for confidence ratings.
 It can be said that current state-of-art techniques are the AIC and BIC
 indices, which evaluate flexibility primarily on the number of a models
 parameter.
 This does not take into account that parameter can vary in their influence
 to the predicted data, such as cognitive processes can vary in their influence
 to observed data.
 AIC and BIC overly punish models which have parameters that yield only
 small influences to the predicted data and thus selections made on the
 basis of AIC and BIC tend to choose models which have parameters that have
 great influence on predicted data.
\end_layout

\begin_layout Standard
Regarding the two models under consideration in this work this seems to
 be the case.
 The discrete-state models, such as the 1HTM, constantly yield good but
 worse fit than the continuous models, such as EVSDT.
 Having the same number of parameters AIC and BIC turn out in favor of of
 the continuous models.
 Summarizing these results many concluded that discrete-state models do
 not account adequately for recognition memory data 
\begin_inset CommandInset citation
LatexCommand citep
key "yonelinas_receiver_2007,wixted_dual-process_2007"

\end_inset

.
 But as we know from more sophisticated flexibility measures 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g. "
key "klauer_flexibility_2011"

\end_inset

 discrete state models are less flexible in their functional form - so it
 might be that the lack of fit is due to the smaller flexibility.
 So it could be that the actual underlying processes are superiorly captured
 by discrete-state models and the predominance of continuous models is due
 to their ability to fit random data.
 Recent findings underpin these considerations: 
\begin_inset CommandInset citation
LatexCommand citet
key "broder_recognition_2009"

\end_inset

 showed that for payoff or base-rate manipulated response biases discrete-state
 models fare better then continuous models.
 For confidence ratings 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 showed that the tenuous assumption that reaching a detect state leads to
 highest confidence responses (certainty assumption) could be the reason
 for discrete-state models' inferiority.
 They suggest 
\emph on
conditional independence
\emph default
, that is once a certain state is entered the distribution for responses
 is determined.
 Using this principle they showed strong evidence in favor of discrete-state
 models.
\end_layout

\begin_layout Subsection
Interpreting recent findings
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

's
\begin_inset CommandInset citation
LatexCommand citeyearpar
key "province_evidence_2012"

\end_inset

 paper was groundbreaking for the new consideration of discrete-state models
 for recognition memory.
 But the used methodology has some peculiarities which complicate comparing
 their results with earlier findings.
 Namely their use of continuous rating scales with post-hoc binning in experimen
t one and two.
 The asymmetric payoff scheme, which aimed to let participants scale their
 responses more towards the center, with immediate feedback in experiment
 two.
 Usage of highly associated words as distractors in the 2AFC task in experiment
 three.
 Although it is unclear in which way these features contributed to their
 results, theoretically the conditional independence should be sufficient.
 The main goal of this thesis is thus to replicate 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "province_evidence_2012"

\end_inset

's findings with a more comparable paradigm.
\end_layout

\begin_layout Standard
Additionally experimental manipulation of the assumed processes should on
 one hand be reflected in the corresponding parameters and on the other
 hand can decrease models with wrong assumptions fit.
 Assuming that the increased fit should also be attributed to the correctness
 of the model, encoding strength as well as mapping is manipulated.
\end_layout

\begin_layout Subsection
Hypothesis
\end_layout

\begin_layout Enumerate
Manipulation of encoding strength shows increase in correct responses.
 These changes should be captured by memory performance parameters (congruent
 validity) of the models under consideration and should not influence any
 other parameters divergent validity.
 Thus, for the two different encoding strengths in all experiments it is
 hypothesized to change the memory parameter (i.e.
 
\begin_inset Formula $\mu$
\end_inset

, 
\begin_inset Formula $d$
\end_inset

 in EVSTD and 1HTM, respectively).
 (No influence on others? Test?)
\end_layout

\begin_layout Enumerate
Manipulation of the response scale should affect the mapping of responses.
 These changes should be captured by response mapping parameters (congruent
 validity) of the models under consideration and not influence any other
 parameter (divergent validity).
 Thus, for the two dofferent response scales in the first sessions it is
 hypothesized to change the response mapping parameter (i.e.
 
\begin_inset Formula $c_{1},\ldots,c_{\frac{n}{2}-1},c_{\frac{n}{2}+1},\ldots,c_{n-1}$
\end_inset

; 
\begin_inset Formula $p_{d,1},\ldots,p_{d,\frac{n}{2}-1},\ldots,p_{g_{r},\frac{n}{2}-1}$
\end_inset

 in EVSDT and 1HTM, respectively).
 Guessing (i.e.
 
\begin_inset Formula $c_{\frac{n}{2}}$
\end_inset

, 
\begin_inset Formula $g_{r}$
\end_inset

 in EVSDT and 1HTM, respectively) bias should be unaffected.
\end_layout

\begin_layout Enumerate
Better fit for 1HTM found by
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

should be replicated.
 1HTM model should yield better fit than EVSDT
\end_layout

\begin_layout Standard
Explorative:
\end_layout

\begin_layout Enumerate
Differences in memory parameters within subject between words and line drawings
 as memorized items.
 Are there differences in other parameters as well?
\end_layout

\begin_layout Enumerate
Role of the zero repetition trials, that are responses to trials that had
 two new words in an 2AFC trial.
\end_layout

\begin_layout Enumerate
Model mimicry of EVSDT and 1HTM
\end_layout

\begin_layout Enumerate
Model landscaping of EVSDT and 1HTM
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
Is it the maximal probability for the observed data? If so, then the probability
 estimates from observed data is the best.
 But they are of little interest for other data then the particular dataset
 they were obtained from.
 
\end_layout

\begin_layout Plain Layout
Disentangeling relevant processes is key.
 -> generalizable, paremter of relevance, 
\end_layout

\begin_layout Plain Layout
So being a good model has something to do with generalizability.
 But what? Or what is generalizability? Is it the predictions for other
 data sets? In general yes, but as parameter estimates are allowed to vary
 these predictions are very broad.
 So we need smaller confidence intervals of the parameter? that certainly
 makes sense.
 
\end_layout

\begin_layout Plain Layout
Is it how the models extract the underlying processes from the data? How
 can we messure how good that is -> parameter validation.
 
\end_layout

\begin_layout Plain Layout
Models should also condense data.
 Why? Because the more flexible a model is the greater the probability of
 having irrelevant parameter.All parameters should be properly validated
 -> scale manipulation for response mapping.
\end_layout

\begin_layout Plain Layout
The risk of having irrelevant parameters could be extended to functional
 form of the model.
 the number of parameters is not alone crucial to the model flexibility.
 
\end_layout

\begin_layout Plain Layout
Mode flexibility is difficult to measure.
 MDL being mathematically the best method.
 model landscaping, model mimicry are indicators for flexibility in comparison
 to other models.
 
\end_layout

\begin_layout Plain Layout
model flexibility is especially important when models with good fit are
 also assumed to be more flexible.
\end_layout

\begin_layout Plain Layout
Very convincing arguments are when the less flexible model obtains better
 fits.
 Especially when the parameters are valid - but because parameter assumptions
 can be wrong that is less important.
\end_layout

\begin_layout Plain Layout
this is what province rouder showed.
\end_layout

\begin_layout Plain Layout
Bot presented models have, so far, proofed to be good account for recognition
 memory data.
 Models are designed to disentangle cognitive processes' underlying observed
 data.
\end_layout

\begin_layout Plain Layout
Model appropriateness is often based on model fit and parsimony.
\end_layout

\begin_layout Plain Layout
SDT models are more flexible but also fit the data better.
\end_layout

\begin_layout Plain Layout
but newer arguments challenge these findings 
\begin_inset CommandInset citation
LatexCommand cite
key "broder_recognition_2009"

\end_inset

 and these arguments are supported by emprical data from 
\begin_inset CommandInset citation
LatexCommand cite
key "province_evidence_2012"

\end_inset

.
\end_layout

\begin_layout Plain Layout
...
\end_layout

\begin_layout Plain Layout
Experimental manipulations of the assumed processes should map to the to
 the respective parameters of the model.
\end_layout

\begin_layout Plain Layout
In recognition memory models response processes are manipulated (Citations)
\end_layout

\begin_layout Plain Layout
This can be done by obtaining confidence ratings for the classifications:
 the proportion of hits per false alerts increases with higher confidence.
 That is participants are more conservative with their answers with increasing
 confidence.
\end_layout

\begin_layout Plain Layout
two alternative forced choice (2AFC) rating scales are a straightforward
 extension to this principle.
 
\end_layout

\begin_layout Plain Layout
Response mapping manipulation
\end_layout

\begin_layout Plain Layout
Encoding strength manipulation via time or repeated presentation+
\end_layout

\begin_layout Plain Layout
Description of models
\end_layout

\begin_layout Plain Layout
All models are making strong assumptions about underlying and unobservable
 processes generating the observable data.
 Additionally to the likelihood for the found data given a specific model
 it is necessary to evaluate the models ability to appropriately reacting
 to treatment manipulation aiming at specific parameters of the models.
\end_layout

\begin_layout Plain Layout
Models differ in their ability to account for data
\end_layout

\begin_layout Plain Layout
Necessity for more data point (ROC plots)
\end_layout

\begin_layout Plain Layout
Options: besrate payoff responce scale
\end_layout

\begin_layout Plain Layout
Responce scale options yes/no vs.
 2AFC
\end_layout

\begin_layout Plain Layout
models
\end_layout

\begin_layout Plain Layout
.
 
\end_layout

\begin_layout Plain Layout
In recognition memory research it is possible to categorize the models in
 three different classes: 
\emph on
discrete-state models
\emph default
, 
\emph on
continuous models,
\emph default
 and 
\emph on
combined models
\emph default
.
 These classes of models are not specific to recognition memory, but are
 applied to a wide field of cognitive research.
 Namely their use of continuous and post-hoc binned rating scales in experiment
 one and two, the asymmetric response scale manipulation in experiment two
 and the use of highly associated words as distractors in the 2AFC task
 in experiment three complicate the interpretation of their results.
 The central research goal in this work is thus to replicate their findings
 with a more comparable design.
\end_layout

\begin_layout Subsection
(Fitting process)
\end_layout

\begin_layout Plain Layout
Model fitting means finding the parameters for the models that maximise
 the likelyhood of the found data.
\end_layout

\begin_layout Plain Layout
The models parameter can be estimated basted on the observed data.
\end_layout

\begin_layout Plain Layout
Estimation is based on maximizing the likelihood for the the given data
 under the fitted model.
\end_layout

\begin_layout Plain Layout
need for a likelihood function for data and model parameter 
\begin_inset Formula $L(D;\theta)$
\end_inset


\end_layout

\begin_layout Plain Layout
maximizing the likelihood function leads to maximum likelihood parameter
 estimates 
\begin_inset Formula $\hat{\theta}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $L(D;\hat{\theta})$
\end_inset

 is the goodness of fit indicator for that model.
\end_layout

\begin_layout Plain Layout
Good fit can be achieved by the appropriateness of the model or by flexible
 models.
\end_layout

\begin_layout Subsection
Model Fit
\end_layout

\begin_layout Plain Layout
Model fit is the likelihood for the data under the best parameter estimates
 of the model.
 
\end_layout

\begin_layout Plain Layout
A better model fit means that the model is the better account for the observed
 data pattern.
\end_layout

\begin_layout Plain Layout
But models 
\end_layout

\begin_layout Plain Layout
fitting process
\end_layout

\begin_layout Plain Layout
model comparison
\end_layout

\begin_layout Plain Layout
latent strength vs.
 discrete state
\end_layout

\begin_layout Plain Layout
genereal
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 are great! 
\end_layout

\begin_layout Plain Layout
weaknesses of 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset


\end_layout

\begin_layout Plain Layout
hypothesis for this thesis
\end_layout

\begin_layout Plain Layout
Province and Rouder's
\begin_inset CommandInset citation
LatexCommand citeyearpar
key "province_evidence_2012"

\end_inset

 results show evidence against the widely accepted superiority of continuous
 models.
 But on the other hand they used some specialities in their studies which
 decrease comparability of their results considerably.
 
\end_layout

\begin_layout Plain Layout
Role of the forced guessing condition
\end_layout

\begin_layout Plain Layout
scale manipulation
\end_layout

\begin_layout Plain Layout
Recognition memory is the ability to recognize previously seen items as
 such.
 The central principle is that there are four response options: hit, false
 alert, correct rejection and miss.
 It is not trivial to separate actual memory performance from the tendency
 to answer in a specific way.
 It is necessary to determine the way these processes are interlinked to
 produce the observable result.
 This is done using mathematical models linking different treatments to
 the respective outcomes.
 
\end_layout

\begin_layout Plain Layout
One class of models are the discrete state models
\end_layout

\begin_layout Plain Layout
2AFC if both are detected as new? What to answer?
\end_layout

\begin_layout Plain Layout
Pictures are easier to learn and recognize than words (Paivio, 1971, 1986).
\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The best model is not necessarily the one that better s the data, as it
 is also important that a model's range of predictions closely follows the
 observations made and that it can produce accurate predictions regarding
 future observations (Roberts & Pashler, 2000) (for discussions on derent
 model selection approaches, see Myung, Forster, & Browne, 2000;Wagenmakers
 & Waldorp, 2006),
\end_layout

\begin_layout Chunk

<<Demographics, echo=FALSE>>= 
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Experiment 1
\end_layout

\begin_layout Subsubsection
Subjects
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Standard
The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{nrow(demographics.df)}
\end_layout

\end_inset

 participants with were recruited in courses, via email list, and bulletin
 offer.
 Participants were offered VP hours
\begin_inset Foot
status open

\begin_layout Plain Layout
Versuchspersonen Stunden: Psychology students have to take part in research
 and get in exchange a recipe for their expenditure of time.
\end_layout

\end_inset

, their individual results, a copy of the completed thesis, and the merit-based
 chance of getting 50€, 30€ or 20€ as the 1st, 2nd, and 3rd place, respectively.
 The sample consisted mostly of young (
\emph on
Mdn
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{median(demographics.df$age)}
\end_layout

\end_inset

, range=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{min(demographics.df$age)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{max(demographics.df$age)}
\end_layout

\end_inset

) psychology (
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{length(grep("(sycholo|SYCHOLO)", demographics.df$occupdesc))}
\end_layout

\end_inset

) students (
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{length(demographics.df$occup[demographics.df$occup=="student"])}
\end_layout

\end_inset

).
 The first 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{nrow(subset(demographics.df, group=="normal"))}
\end_layout

\end_inset

 subjects were assigned to Group 1 
\begin_inset Quotes eld
\end_inset

fully encoded
\begin_inset Quotes erd
\end_inset

 the rest (
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{nrow(subset(demographics.df, group=="forcedguess"))}
\end_layout

\end_inset

) were assigned to Group 2 
\begin_inset Quotes eld
\end_inset

forced guessing
\begin_inset Quotes erd
\end_inset

.
 Each participant were invited for two session: Session 1 with verbal items
 and session 2 with visual items.
\end_layout

\begin_layout Subsubsection
Materials
\end_layout

\begin_layout Standard
The paradigm was implemented as Python program.
 Verbal stimuli for the first sessions of bot groups were neutral german
 nouns taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "lahl_using_2009"

\end_inset

 ranging from 4 to 8 letters in length.
 According to the ratings obtained by 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "lahl_using_2009"

\end_inset

, the words were all of medium valence (ranging from 3.5 to 6.5 on an 11-point
 scale) and low in arousal (ranging from 0.5 to 4.5 on an 11-point scale).
 For Group 1 I used 644 nouns that were of approximately equal word frequency
 in common German, as indicated by log frequency ratings obtained for each
 word via WordGen 
\begin_inset CommandInset citation
LatexCommand citep
before "ranging from 0.3 to 2.9;"
key "duyck_wordgen:_2004"

\end_inset

.
 For Group 2 I used 857 nouns, that were of approximately equal word frequency
 in common German, as indicated by frequency classes obtained for each word
 from DeReWo
\begin_inset CommandInset citation
LatexCommand citep
before "ranging from 10 to 16; "
key "institut_fur_deutsche_sprache_programmbereich_korpuslinguistik_korpusbasierte_2013"

\end_inset

.
 Visual stimuli for both sessions were 525 black and white line drawings
 taken from 
\begin_inset CommandInset citation
LatexCommand cite
key "szekely_timed_2003"

\end_inset

.
\end_layout

\begin_layout Standard
Responses were collected using 8-steps and 6-steps two alternative forced
 choice (2AFC) rating scales in the in the in the fully encoded group and
 the forced guessing group, respectively.
 Each side of the scale was labeled 
\emph on
left
\emph default
 and 
\emph on
right
\emph default
, respectively and vertically centered on the screen and horizontally centered
 above each half of the scale one item was shown.
 The two central buttons on each side were labeled with the character 
\emph on
1
\emph default
 and the more peripheral buttons were labeled with increasing numbers (see
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rating-scales"

\end_inset

).
 In the fully encoded group either the left or the right item of each trial
 has been shown before the other one was a lure.
 In the forced guessing group where trials added in that none of the two
 items has been shown before (i.e.
 the answer could not be based on recognition of an old item).
 In the first sessions of both groups, in which verbal items where to be
 memorized, the numbers more peripheral numbers increased either by one
 or by four for the 
\begin_inset Quotes eld
\end_inset

low-risk
\begin_inset Quotes erd
\end_inset

, and 
\begin_inset Quotes eld
\end_inset

high-risk
\begin_inset Quotes erd
\end_inset

 rating scales, respectively, which resulted in 1, 2, 3, 4; 1, 5, 9, 13
 for the fully encoded group and 1, 2, 3; 1, 5, 9 for the forced guessing
 group each low-risk and high-risk scales, respectively.
\end_layout

\begin_layout Standard
Each participant was asked to generate an individual VP-code based on a
 given structure for anomy identification.
 Socio-demographic data was collected subsequently to each session.
 In order to be able to get the monetary reward and their individual data,
 participants had to give their e-mail addresses.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Rating scales
\begin_inset CommandInset label
LatexCommand label
name "fig:Rating-scales"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Procedures
\end_layout

\begin_layout Subsubsection
Design
\end_layout

\begin_layout Standard
The main treatment factors were the encoding strength that is how often
 a word has been shown in the learning phase, the position of the learned
 item in the 2AFC-rating task, the response scale used in the 2AFC task.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:experimental-factors"

\end_inset

shows all factor combinations, resulting data cells and degrees of freedom.
 All factor combinations are applied in a within subject factorial design.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Tadaa
\begin_inset CommandInset label
LatexCommand label
name "tab:experimental-factors"

\end_inset


\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="13">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Grp.
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Session
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
encoding
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
positions
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
scale
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
factor comb.
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
steps
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
cells
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
df
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
levels
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
trials
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Verbal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 4x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low, high risk)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
36
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
56
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Visual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 3x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low risk)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Verbal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(0x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(None)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low, high risk)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
36
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 4x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Visual
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(0x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(None)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="left" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(low risk)
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1x, 3x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(L, R)
\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multirow="4" alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiment 2
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The observed frequency for the confidence levels under each unique factor
 combination is calculated.
 The obtained category frequencies are then fit in R 
\begin_inset CommandInset citation
LatexCommand citep
key "r_core_team_r:_2013"

\end_inset

 using the package MPTinR
\begin_inset CommandInset citation
LatexCommand citep
key "singmann_mptinr:_2013"

\end_inset

 to the unrestricted and various restricted versions of the MPT1HTM and
 EVSDT models.
 Internally this is done via the 
\begin_inset Flex Code
status collapsed

\begin_layout Plain Layout
nlminb
\end_layout

\end_inset

 function that uses the PORT optimization routine 
\begin_inset CommandInset citation
LatexCommand citep
key "gay_usage_1990"

\end_inset

.
 The obtained fits are then evaluated using 
\begin_inset Formula $G^{2}$
\end_inset

-values, that are 
\begin_inset Formula $\chi^{2}$
\end_inset

-distributed with the difference of the number of free data points and the
 number of parameters of the model as df and thus allow null-hypothesis
 testing.
 For nested models (i.e.
 models which are obtained via equality restriction parameters) the difference
 in 
\begin_inset Formula $G^{2}$
\end_inset

-values is also 
\begin_inset Formula $\chi^{2}$
\end_inset

-distributed with the difference in parameters as df 
\begin_inset CommandInset citation
LatexCommand citep
key "riefer_multinomial_1988"

\end_inset

.
 For evaluating the overall fit of a model across participants, the sum
 of all individual 
\begin_inset Formula $G^{2}$
\end_inset

-values is 
\begin_inset Formula $\chi^{2}$
\end_inset

-distributed with the sum of all individual df.
\end_layout

\begin_layout Chunk

<<DataSetup, cache=TRUE, echo=FALSE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
First the effects of the experimental manipulations are presented for each
 experiment individually.
 Then the equivalent models (plus scale manipulation) to the ones used by
 
\begin_inset CommandInset citation
LatexCommand citet
key "province_evidence_2012"

\end_inset

 are compared.
 Lastly the stability of each participants superior model across words and
 pictures as memorized items is presented.
\end_layout

\begin_layout Subsection
Experiment 1
\end_layout

\begin_layout Chunk

<<LoadExp1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The most general MPT1HTM model provided a good account for both sessions
 and summed across all participants 
\begin_inset Formula $G^{2}$
\end_inset

(
\end_layout

\begin_layout Subsubsection
Effects of manipulation
\end_layout

\begin_layout Chunk

<<Res01, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
In this experiment repetition of items in the learning phase was varied
 in both sessions.
 In session one additionally two response scales were used.
 In the first session the memorized items were word and in the second pictures.
 First we check if different repetitions influenced memory...
\end_layout

\begin_layout Paragraph
Encoding strength
\end_layout

\begin_layout Standard
The MPT1HTM model with equal memory parameter across both encoding strengths
 restricted to be equal could be rejected 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session 2, respectively.
 This model was also individually rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants, in Session 1 and 2, respectively, with p < .05, thus the overall
 rejection is not based on a few extreme values.
 Different memory parameters for each level of encoding strength are necessary
 to properly account for the data.
 The same pattern, yet even more distinct, is found for the EVSDT model.
 The version with memory parameter restricted to be equal across different
 encoding strengths could be rejected with 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session2, respectively.
 Individually it was rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 participants, in Session 1 and Session 2, respectively.
 Subsequently the version of both models with memory parameter restricted
 to be equal are excluded from further analysis.
\end_layout

\begin_layout Paragraph
Scale
\end_layout

\begin_layout Standard
The models with memory parameter restricted to be equal across the different
 scales used in Session 1 were more parsimonious without reducing fit 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 The null that both models are equal in terms of GoF holds also on individual
 level with 
\emph on
p 
\emph default
< .05 only for
\emph on
 
\emph default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[2]]}
\end_layout

\end_inset

 participants out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Based on these findings further analysis is done using the more parsimonious
 versions of MPT1HTM and EVSDT with memory parameter restricted to be equal
 across scales.
\end_layout

\begin_layout Standard
To check wether the scale manipulation has any influence at all the MPT1HTM
 and EVSDT models are further restricted to have their mapping parameter
 and cutoff criteria, respectively, restricted to be equal across the different
 scales.
 These fully restricted versions which assume no influence of the scale
 manipulation had to be rejected: 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Individually these models were rejected at 
\emph on
p
\emph default
 < .05 for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[2]]}
\end_layout

\end_inset

 participants, for MPT1HTM and EVSDT, respectively.
\end_layout

\begin_layout Subsubsection
Model comparison
\end_layout

\begin_layout Chunk

<<Res02, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The restricted versions of MPT1HTM and EVSDT with the best and parsimonious
 account for the data (i.e.
 the versions with only memory parameters restricted to be equal across
 scales), are compared against each other.
 Since both models have equal number of parameters (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

 for Session 1, 2 respectively) classical selection criteria like AIC and
 BIC are merely a function of fit, hence the 
\begin_inset Formula $G^{2}$
\end_inset

-statistics are used directly.
 The differences are not conclusive (
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Median"], digits=2)}
\end_layout

\end_inset

, range: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Max."], digits=2)}
\end_layout

\end_inset

, and 
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Median"], digits=2)}
\end_layout

\end_inset

, range:
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Max."], digits=2)}
\end_layout

\end_inset

, for Session 1 and 2, respectively, greater values imply preference for
 MPT1HTM).
 Individually accounted for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses1[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses2[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants in Session 1 and 2, respectively, the MPT1HTM model better
 for the data.
 This further supports the overall impression that the models in this experiment
 performed approximately equally well.
\end_layout

\begin_layout Chunk

<<Res02p1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<Res02p2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsubsection
Stability of individual best model
\end_layout

\begin_layout Chunk

<<Res03, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
Session 1 and Session 2 has been completed by 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.both.ses"]}
\end_layout

\end_inset

 participants.
 In each session every participant's data was best captured with either
 the MPT1HTM or the EVSDT model.
 Across session this best model was stable for 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.stable"]}
\end_layout

\end_inset

 participants and thus just about the randomly expected rate of 50%.
\end_layout

\begin_layout Subsection
Experiment 2
\end_layout

\begin_layout Chunk

<<LoadExp2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The most general MPT1HTM model provided a good account for both sessions
 and summed across all participants 
\begin_inset Formula $G^{2}$
\end_inset

(
\end_layout

\begin_layout Subsubsection
Effects of manipulation
\end_layout

\begin_layout Chunk

<<Res01, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
In this experiment repetition of items in the learning phase was varied
 in both sessions.
 In session one additionally two response scales were used.
 In the first session the memorized items were word and in the second pictures.
 First we check if different repetitions influenced memory...
\end_layout

\begin_layout Paragraph
Encoding strength
\end_layout

\begin_layout Standard
The MPT1HTM model with equal memory parameter across both encoding strengths
 restricted to be equal could be rejected 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session 2, respectively.
 This model was also individually rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.enc)","p.smaller.05"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants, in Session 1 and 2, respectively, with p < .05, thus the overall
 rejection is not based on a few extreme values.
 Different memory parameters for each level of encoding strength are necessary
 to properly account for the data.
 The same pattern, yet even more distinct, is found for the EVSDT model.
 The version with memory parameter restricted to be equal across different
 encoding strengths could be rejected with 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","df.sum"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","G.Squared.sum"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.sum"])}
\end_layout

\end_inset

, for Session 1 and Session2, respectively.
 Individually it was rejected for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="EVSDT(res.mu.enc)","p.smaller.05"]}
\end_layout

\end_inset

 participants, in Session 1 and Session 2, respectively.
 Subsequently the version of both models with memory parameter restricted
 to be equal are excluded from further analysis.
\end_layout

\begin_layout Paragraph
Scale
\end_layout

\begin_layout Standard
The models with memory parameter restricted to be equal across the different
 scales used in Session 1 were more parsimonious without reducing fit 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1mpt[[1]][comp1mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "G.Squared.diff"], digits=2)}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp1sdt[[1]][comp1sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 The null that both models are equal in terms of GoF holds also on individual
 level with 
\emph on
p 
\emph default
< .05 only for
\emph on
 
\emph default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp1sdt[[2]]}
\end_layout

\end_inset

 participants out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Based on these findings further analysis is done using the more parsimonious
 versions of MPT1HTM and EVSDT with memory parameter restricted to be equal
 across scales.
\end_layout

\begin_layout Standard
To check wether the scale manipulation has any influence at all the MPT1HTM
 and EVSDT models are further restricted to have their mapping parameter
 and cutoff criteria, respectively, restricted to be equal across the different
 scales.
 These fully restricted versions which assume no influence of the scale
 manipulation had to be rejected: 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

, 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2mpt[[1]][comp2mpt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

 and 
\begin_inset Formula $G^{2}$
\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "df.diff"]}
\end_layout

\end_inset

) = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "G.Squared.diff"])}
\end_layout

\end_inset

 
\emph on
p
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{print.p(comp2sdt[[1]][comp2sdt[[1]]$code=="sum", "p"])}
\end_layout

\end_inset

, for MPT1HTM and EVSDT, respectively.
 Individually these models were rejected at 
\emph on
p
\emph default
 < .05 for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2mpt[[2]]}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{comp2sdt[[2]]}
\end_layout

\end_inset

 participants, for MPT1HTM and EVSDT, respectively.
\end_layout

\begin_layout Subsubsection
Model comparison
\end_layout

\begin_layout Chunk

<<Res02, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
The restricted versions of MPT1HTM and EVSDT with the best and parsimonious
 account for the data (i.e.
 the versions with only memory parameters restricted to be equal across
 scales), are compared against each other.
 Since both models have equal number of parameters (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses1[select.ses1$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{select.ses2[select.ses2$model=="MPT1HTM(res.d.scale)", "n.parameters"]}
\end_layout

\end_inset

 for Session 1, 2 respectively) classical selection criteria like AIC and
 BIC are merely a function of fit, hence the 
\begin_inset Formula $G^{2}$
\end_inset

-statistics are used directly.
 The differences are not conclusive (
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Median"], digits=2)}
\end_layout

\end_inset

, range: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses1["Max."], digits=2)}
\end_layout

\end_inset

, and 
\emph on
Mdn
\emph default
: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Median"], digits=2)}
\end_layout

\end_inset

, range:
 
\lang american

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Min."], digits=2)}
\end_layout

\end_inset

-
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{round(sum.diff.ses2["Max."], digits=2)}
\end_layout

\end_inset

, for Session 1 and 2, respectively, greater values imply preference for
 MPT1HTM).
 Individually accounted for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses1[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{diff.ses2[[2]]["n.wins_MPT1HTM(res.d.scale)"]}
\end_layout

\end_inset

 out of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses1}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{n.part.ses2}
\end_layout

\end_inset

 participants in Session 1 and 2, respectively, the MPT1HTM model better
 for the data.
 This further supports the overall impression that the models in this experiment
 performed approximately equally well.
\end_layout

\begin_layout Chunk

<<Res02p1, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

<<Res02p2, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsubsection
Stability of individual best model
\end_layout

\begin_layout Chunk

<<Res03, cache=FALSE, echo=TRUE, eval=TRUE>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Standard
Session 1 and Session 2 has been completed by 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.both.ses"]}
\end_layout

\end_inset

 participants.
 In each session every participant's data was best captured with either
 the MPT1HTM or the EVSDT model.
 Across session this best model was stable for 
\emph on
n
\emph default
=
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{stab.diff[[2]]["n.stable"]}
\end_layout

\end_inset

 participants and thus just about the randomly expected rate of 50%.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Standard
First, the individual probability distributions for each condition is calculated.
\end_layout

\begin_layout Standard
\begin_inset Formula $x\in\{x\vert x\in\mathbb{Z}\wedge x\geq=0\wedge x<\text{\#points of rating scale}\}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(X=x\vert e\in E,\, s\in S,\, p\in P)=\begin{cases}
d_{e}\prod_{k=x+1}^{k<5}(1-dd_{sk})\prod_{k=x}^{3<k\leq x}(dd_{sk})+(1-d{}_{e})g_{p}\prod(1-gd_{s})\prod(gd_{s})\end{cases}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
m_{Gij}=\begin{cases}
\theta j & \text{if }j=i<\left|I\right|\\
1-\theta j & \text{if }j>i\\
1 & \text{else}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\]

\end_inset


\end_layout

\begin_layout Standard
Zero repetition
\end_layout

\begin_layout Standard
Experimental situation
\end_layout

\begin_layout Standard
Strong memories are hard to scale 
\begin_inset CommandInset citation
LatexCommand citep
key "mickes_strong_2011"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "MasterThesis"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Appendices
\end_layout

\begin_layout Subsection
Fit Table
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Subsection
Participants' plots
\end_layout

\begin_layout Chunk

<<AllPlots, eval=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, fig.width=21*0.75,
 fig.height=29*0.75>>=
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Data Manipulation R-code
\end_layout

\begin_layout Chunk

<<DataManipulation.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<DataManipulation>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Model Fitting R-code
\end_layout

\begin_layout Chunk

<<ModelFitting.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<ModelFitting>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Model specifications R-code
\end_layout

\begin_layout Chunk

<<ModelSpecifications.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<ModelSpecifications>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Subsection
Plotting R-code
\end_layout

\begin_layout Chunk

<<Plotting.echo, eval=FALSE, echo=TRUE>>=
\end_layout

\begin_layout Chunk

<<Plotting>>
\end_layout

\begin_layout Chunk

@
\end_layout

\begin_layout Chunk

\end_layout

\end_body
\end_document
